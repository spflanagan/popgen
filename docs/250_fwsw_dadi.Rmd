---
title: "FWSW analysis with dadi"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.extra='',fig.pos="H")
knitr::opts_knit$set(root.dir='./fwsw_results/')
```
```{r source}
source("../../gwscaR/R/gwscaR.R")
source("../../gwscaR/R/gwscaR_plot.R")
source("../../gwscaR/R/gwscaR_utility.R")
source("../../gwscaR/R/gwscaR_fsts.R")
source("../../gwscaR/R/gwscaR_popgen.R")
source("../../gwscaR/R/vcf2dadi.R")
source("../R/250_dadi_analysis.R")
library(knitr)
pop.list<-c("ALFW","ALST","FLCC","FLLG","LAFW","TXCC","TXFW")
```

Some useful functions are in the `250_dadi_analysis.R` script.

# Create a dadi SNPs file from the vcf

```{r create_dadi, eval=FALSE}
vcf<-parse.vcf("stacks/populations_subset75/batch_2.vcf")
dadi<-vcf2dadiSNPs(vcf,pop.list = pop.list,filename = "dadi_analysis/fwsw.dadi.snps")
dadi<-read.delim("dadi_analysis/fwsw.dadi.snps")
projections<-unlist(lapply(pop.list,function(pop){ n<-length(grep(pop,colnames(vcf)))}))*2


names(projections)<-pop.list
best.pops<-c("TXFW","TXCB","LAFW","ALST","ALFW","FLLG","FLCC","TXFW ","TXCB ","LAFW ","ALST ","ALFW ","FLLG ","FLCC ")
best.pops<-c("TXFW","TXCB","LAFW","ALST","ALFW","FLLG","FLCC","TXFW.1","TXCB.1","LAFW.1","ALST.1","ALFW.1","FLLG.1","FLCC.1")
dadi<-dadi[,c("Ingroup","Outgroup","Allele1",best.pops[1:7],"Allele2",best.pops[8:14],"GeneID","Position")]
dim(dadi[rowSums(dadi[,colnames(dadi) %in% best.pops])==sum(projections[best.pops[1:7]]),])

#transform this and remove everything but population info for ease of operations
dd<-as.data.frame(cbind(colnames(dadi)[colnames(dadi) %in% best.pops],t(dadi[,colnames(dadi) %in% best.pops])),row.names = 0,stringsAsFactors = FALSE)
#rename the pops to be factors, essentially
dd$V1<-gsub("\\.1","",dd$V1)
#calculate sums per locus for each population
ns<-data.frame(do.call(rbind,lapply(2:ncol(dd),function(c){ tapply(dd[,c],dd$V1,function(x){ sum(as.numeric(x)) }) })))
rownames(ns)<-dadi$GeneID
nozeros<-ns[!(apply(ns, 1, function(y) any(y == 0))),] #512 had zeroes
props<-apply(nozeros,2,function(x) { p<-x/max(x) } )
keepers<-props[apply(props, 1, function(y) any(y >=0.75)),]

#now save these in the dadi format
write.table(dadi[dadi$GeneID %in% rownames(keepers),],"dadi_analysis/fwsw.dadi.pruned.snps",quote=FALSE,row.names = FALSE,
            col.names = c("Ingroup","Outgroup","Allele1","TXFW","TXCB","LAFW","ALST","ALFW","FLLG","FLCC","Allele2","TXFW","TXCB","LAFW","ALST","ALFW","FLLG","FLCC","GeneID","Position"))
```

```{r check_dadis}
vcf.files<-c("stacks/populations_subset50/batch_2.vcf",
             "stacks/populations_subset75/batch_2.vcf",
             #"stacks/populations_subset85/batch_2.vcf",
             #"stacks/populations_subset90/batch_2.vcf",
             #"stacks/populations_subset95/batch_2.vcf",
             "stacks/populations_subset100/batch_2.vcf")

dadis<-lapply(vcf.files,function(file){
  full.vcf<-parse.vcf(file)
  vcf<-choose.one.snp(full.vcf)
  vcf.out.name<-gsub("batch_2.vcf","batch_2.pruned.vcf",file)
  write.table(x = vcf,file = vcf.out.name,quote = FALSE,sep='\t',row.names = FALSE,col.names = TRUE)
  dadi.out<-paste("dadi_analysis/fwsw",gsub(".*populations_subset(\\d+).*","\\1",file),".dadi.snps",sep="")
  pop.list<-c("ALFW","ALST","FLCC","FLLG","LAFW","TXCC","TXFW")
  dadi<-vcf2dadiSNPs(vcf,pop.list = pop.list,filename = dadi.out)
  return(dadi)
})

#compare
kable(cbind(file=vcf.files,numer_snps=unlist(lapply(dadis,nrow))))
```

I'm going to use the 75% of individuals dataset.

## Analyze 1D dadi results

I ran the optimize functions from (dportik's dadi pipeline)[https://github.com/dportik/dadi_pipeline] for each of the 7 populations. Now I need to see what the optimal parameters are (with maximum log likelihood).


```{r}
opt.params<-do.call(rbind,lapply(pop.list, function(pop,dadi.dir="dadi_analysis"){
  if(length(grep("/^",dadi.dir))==0){
    dadi.dir<-paste(dadi.dir,"/",sep="")
  }
  path<-paste(dadi.dir,pop,sep="")
  opts<-dadi.modelcomp(path=path,pattern="optimized")
  return(opts)
}))
write.table(opt.params,"dadi_analysis/1D_optimal_parameters.txt",sep='\t',quote=FALSE,col.names = TRUE,row.names = FALSE)
```
```{r}
opt.params<-read.delim("dadi_analysis/1D_optimal_parameters.txt",header = TRUE)
kable(opt.params)
```



## Analyze 2D FL optimization results


I ran the optimize functions from (dportik's dadi pipeline)[https://github.com/dportik/dadi_pipeline] for a set of 2D demographic models for the Florida populations (FLFW and FLCC):

* Split into two populations, no migration (distributed with `dadi-pipeline`)
* Split into two populations, with continuous symmetric migration (distributed with `dadi-pipeline`)
* Split into two populations, with continuous asymmetric migration (distributed with `dadi-pipeline`)
* Split into two pops, isolation with migration model (distributed with `dadi`)
* Split into two pops, growth in one pop and two epoch in another, no migration (custom model specified in `scripts/250_custom_dadi_models.py`)
* Split into two pops, growth in one pop and two epoch in another, symmetric migration (custom model specified in `scripts/250_custom_dadi_models.py`)
* Split into two pops, growth in one pop and two epoch in another, asymmetric migration (custom model specified in `scripts/250_custom_dadi_models.py`)

At least some of the runs in all of these do not work well (result in "WARNING:Numerics:Extrapolation may have failed. Check resulting frequency spectrum for unexpected results; WARNING:Inference:Model is masked in some entries where data is not"), but the output will tell me more about how frequently it occurs (by parameters showing up as 'nan' instead of values)

Now I need to see what the optimal parameters are for these 7 different runs so that I can run those models for additional iterations and/or compare model fits -- after which I will simulate data with `ms`.

```{r}
fl.opts<-dadi.modelcomp(path = "dadi_analysis/FL2D",pattern="V1.*optimized.*",id="FL2D")
kable(fl.opts,caption = "'Best' runs of each of the models for the Florida populations 2D demographic inference")
```

This doesn't tell us about which models had trouble converging, though, so let's tally up the number of 'nan's in each file.

```{r}
fl.nans<-do.call(rbind,lapply(list.files(path = "dadi_analysis/FL2D",pattern="V1.*optimized.*",full.names = TRUE),dadi.nan))
```

How about the ones with warnings? (some of my models did not run, so I'm ignoring those for now. )

```{r}
fl.warnings<-dadi_warnings("dadi_analysis/252_2DFL_V1_1.log",mods=c("Model no_mig","Model sym_mig","Model asym_mig","Model IM"))
fl.warnings$Warning<-TRUE
fl.opts<-do.call(rbind,lapply(list.files(path = "dadi_analysis/FL2D",pattern="V1_Number_1.*[mM].*optimized.*",full.names = TRUE),parse_dadi_opt))
fl.dat<-merge(fl.opts,fl.warnings,by=c("Model","Replicate"),all = TRUE)

```
```{r}
fl.warnings2<-dadi_warnings("dadi_analysis/252_2DFL_V1_2.log")
fl.warnings2$Warning<-TRUE
fl.opts2<-do.call(rbind,lapply(list.files(path = "dadi_analysis/FL2D",pattern="V1_Number_2.*optimized.*",full.names = TRUE),parse_dadi_opt))
fl.dat2<-merge(fl.opts2,fl.warnings2,by=c("Model","Replicate"),all = TRUE)
tapply(fl.warnings2$Warning,fl.warnings2$Model,table)
tapply(fl.warnings$Warning,fl.warnings$Model,table)
```
```{r}
fl.warnings3<-dadi_warnings("dadi_analysis/252_2DFL_V1_3.log")
fl.warnings3$Warning<-TRUE
fl.opts3<-do.call(rbind,lapply(list.files(path = "dadi_analysis/FL2D",pattern="V1_Number_3.*optimized.*",full.names = TRUE),parse_dadi_opt))
fl.dat3<-merge(fl.opts3,fl.warnings3,by=c("Model","Replicate"),all = TRUE)
tapply(fl.warnings3$Warning,fl.warnings3$Model,table)
tapply(fl.warnings2$Warning,fl.warnings2$Model,table)
tapply(fl.warnings$Warning,fl.warnings$Model,table)
```
```{r}
fl.warnings4<-dadi_warnings("dadi_analysis/252_2DFL_V1_4.log")
fl.warnings4$Warning<-TRUE
fl.opts4<-do.call(rbind,lapply(list.files(path = "dadi_analysis/FL2D",pattern="V1_Number_4.*optimized.*",full.names = TRUE),parse_dadi_opt))
fl.dat4<-merge(fl.opts4,fl.warnings4,by=c("Model","Replicate"),all = TRUE)
tapply(fl.warnings4$Warning,fl.warnings4$Model,table)
tapply(fl.warnings3$Warning,fl.warnings3$Model,table)
tapply(fl.warnings2$Warning,fl.warnings2$Model,table)
tapply(fl.warnings$Warning,fl.warnings$Model,table)
```


Most of the warnings come from the IM model. 

```{r}
fl_dat<-extract_dadi_params(rbind(fl.dat,fl.dat2,fl.dat3,fl.dat4))
par(xpd=TRUE,mar=c(2,6,2,2),mfrow=c(length(fl_dat),2))
pl<-lapply(fl_dat,function(dat){ 
  require(scales)
  require(sm)
  params<-colnames(dat)[(which(colnames(dat)=="Warning")+1):ncol(dat)]
  plot_params(dat[!is.na(dat$Warning),],params,pt.col = "coral")
  legend("topleft",paste("N=",nrow(dat[!is.na(dat$Warning),])),bty='n')
  plot_params(dat[is.na(dat$Warning),],params,pt.col = "grey")
  legend("topleft",paste("N=",nrow(dat[is.na(dat$Warning),])),bty='n')
})


```

The ones with warnings are pretty bad. What's the best model if I remove ones with warnings?

```{r}
fl.bests<-lapply(fl_dat,function(dat){ 
  keep<-dat[is.na(dat$Warning),]
  best<-keep[which.max(keep$log.likelihood),]
})
fl.bests[[which.min(unlist(lapply(fl.bests,function(x){ return(x$AIC) })))]]
```

I used the best estimates to improve model fits each round.

Then I used teh best overall fit from 5 rounds to run simulations using dadi-pipeline-master.


Ok, but what about convergence? Are the replicates (Numbers 1-5) converging on likelihoods and parameter values?


```{r}
library(scales)
library(sm)
models<-unlist(lapply(fl_dat,function(dat) unique(dat$Model)))
par(mfrow=c(3,3))
l<-lapply(models,function(mod){
  files<-list.files(path="dadi_analysis/FL2D",pattern = paste(".*\\.",mod,".optimized.*",sep=""),full.names = TRUE)
  all_reps<-do.call(rbind,lapply(files, parse_dadi_opt))
  lines_per_rep<-nrow(all_reps)/length(files)
  all_reps$rep<-rep(1:length(files),each=lines_per_rep)
  #get the parameter values
  params<-do.call(rbind,lapply(all_reps$optimized_params,function(x){
    p<-as.numeric(as.character(rbind(unlist(strsplit(x,",")))))
    return(p)
  }))
  colnames(params)<-unlist(strsplit(unlist(strsplit(all_reps$params[1],",")),"\\."))
  all_reps<-cbind(all_reps,params)
  
  #plot
  par(xpd=TRUE,mar=c(2,6,2,2))
  plot(0:(ncol(params)*2),xlim=c(0,2*(ncol(params))),ylim=c(min(all_reps$log.likelihood),max(all_reps[,colnames(params)])),bty="L",axes=FALSE,ann=FALSE,type='n')
  gwsca.vioplot(all_reps$log.likelihood,col="grey",add=TRUE,xpd=TRUE,at=0)
  points(jitter(rep(0,nrow(all_reps)),2.5),all_reps$log.likelihood,col=alpha(all_reps$rep,0.5),pch=19)
  axis(2,las=1,pos=-0.5)
  axis(1,lwd=0,label="log likelihood",at=0)
  #mapply(function(param,counts) browser(),colnames(params),2:(ncol(params)+1))
  p<-mapply(function(param,counts){
    par(new=TRUE)
    plot(0:(ncol(params)*2),xlim=c(0,2*(ncol(params))),ylim=c(min(all_reps[,param]),max(all_reps[,param])),bty="L",axes=FALSE,ann=FALSE,type='n')
    gwsca.vioplot(all_reps[,param],col="grey",add=TRUE,xpd=TRUE,ylim=c(min(all_reps[,param]),max(all_reps[,param])),at=counts)
    points(jitter(rep(counts,nrow(all_reps)),2.5),all_reps[,param],col=alpha(all_reps$rep,0.5),pch=19)
    axis(2,las=1,pos=counts-0.75)
    axis(1,lwd=0,label=param,at=counts)
  },colnames(params),seq(2,(ncol(params)*2),by=2))
  mtext(mod,3)
  
})
```

### Analyze Florida simulations

```{r}
sim_dirs<-grep("sims",list.dirs(path="dadi_analysis/FL2D",full.names = TRUE),value = TRUE)
sims<-do.call(rbind,lapply(sim_dirs,function(x) {
  dat<-read.delim(paste0(x,"/Simulation_Results.txt"),header=TRUE)
  dat$sim<-x
  params<-do.call(rbind,strsplit(as.character(dat$optimized_params),","))
  params<-apply(params,2,as.numeric)
  dat<-cbind(dat,params)
  return(dat)
}))
emps<-do.call(rbind,lapply(sim_dirs,function(x) {
  dat<-read.delim(paste0(x,"/Empirical.asym_mig.optimized.txt"),header=TRUE)
  dat$sim<-x
  return(dat)
}))

start_params<-c(nu1=0.1317,nu2=8.4225,m12=0.7777,m21=0.0561,T=0.1441)

summary(sims$log.likelihood)
summary(sims$chi.squared)
```

Use distributions of log-likelihood and $\chi^2$ values from simulated datasets to evaluate whether the empircally-estimated values are contained within these distributions (following https://onlinelibrary.wiley.com/doi/full/10.1111/mec.14862). 

```{r}
### FROM dadi_pipeline's Plot_GOF.R
#####################################
#likelihood plot
ll_seq<- seq(-1500,-900,20)
hist(sims$log.likelihood, breaks=ll_seq, main = "Simulation Results - Log-likelihood distribution", xlab="log-likelihood", col="grey")
abline(v=emps$log.likelihood, lwd = 3, col='blue')

#chi-squared plot
chi_seq<- seq(1500,5000,50)
hist(sims$chi.squared, breaks=chi_seq, main = "Simulation Results - Chi-squared distribution ", xlab="Chi-squared test statistic", col="grey")
abline(v=emps$chi.squared, lwd = 3, col='blue')

#####################################
#For log-transformed chi.squared

#transform chi-squared test stat
log_chi <- log(sims$chi.squared)
emp_log_chi <- log(emps$chi.squared)
summary(log_chi)


#log transformed chi-squared plot
lchi_seq<- seq(7,9,0.05)
hist(log_chi, breaks=lchi_seq, main = "Simulation Results - Log Chi-squared distribution ", xlab="log Chi-squared test statistic", col="grey")
abline(v=emp_log_chi, lwd = 3, col='blue')


```

Now let's check the parameter estimates
```{r}
par(mfrow=c(2,3),oma=c(2,2,2,2),mar=c(2,2,2,2))

for(i in 1:length(start_params)){
  hist(sims[,8+i], breaks=50, main = names(start_params)[i], xlab=names(start_params)[i], col="grey")
  abline(v=start_params[i], lwd = 3, col='blue')
}

```


But I would like to possibly model varying rates across the genome, and at the very least use the simulations to help ID outliers. So I'm going to run ms and possibly design some additional dadi models with heterogeneous migration across the genome. 

## Analyze 2D TX optimization results


I ran the optimize functions from (dportik's dadi pipeline)[https://github.com/dportik/dadi_pipeline] for a set of 2D demographic models for the Texas populations (TXFW and TXCC):

* Split into two populations, no migration (distributed with `dadi-pipeline`)
* Split into two populations, with continuous symmetric migration (distributed with `dadi-pipeline`)
* Split into two populations, with continuous asymmetric migration (distributed with `dadi-pipeline`)
* Split into two pops, instantaneous size change and no migration (distributed with `dadi-pipeline`)
* Split into two pops, instantaneous size change and continuous symmetric migration (distributed with `dadi-pipeline`)
* Split into two pops with symmetrical gene flow, instantaneous size change and no migration (distributed with `dadi-pipeline`)
* Split into two pops with asymmetrical gene flow, instantaneous size change and no migration (distributed with `dadi-pipeline`)

At least some of the runs in all of these do not work well (result in "WARNING:Numerics:Extrapolation may have failed. Check resulting frequency spectrum for unexpected results; WARNING:Inference:Model is masked in some entries where data is not"), but the output will tell me more about how frequently it occurs (by parameters showing up as 'nan' instead of values)

Now I need to see what the optimal parameters are for these 7 different runs so that I can run those models for additional iterations and/or compare model fits -- after which I will simulate data with `ms`.

```{r}
tx.best<-dadi.modelcomp(path = "dadi_analysis/TX2D",pattern="V.*optimized.*",id="TX2D")
kable(tx.best,caption = "'Best' runs of each of the models for the Texas populations 2D demographic inference")
```

```{r}
tx.opts<-do.call(rbind,lapply(list.files(path = "dadi_analysis/TX2D",pattern="V2.*optimized.*",full.names = TRUE),parse_dadi_opt))
tx_dat<-extract_dadi_params(tx.opts)
tx_dat<-lapply(tx_dat,function(dat){
  dat$log.likelihood<-as.numeric(as.character(dat$log.likelihood))
  return(dat)
})
par(xpd=TRUE,mar=c(2,6,2,2),mfrow=c(length(tx_dat),1))
pl<-lapply(tx_dat,function(dat){ 
  require(scales)
  require(sm)
  params<-colnames(dat)[(which(colnames(dat)=="params")+1):ncol(dat)]
  plot_params(dat,params,pt.col = "grey")
  legend("topleft",paste("N=",nrow(dat)),bty='n')
})

```


Are the multiple rounds improving the model fit?

```{r}
par(xpd=TRUE,mar=c(2,6,2,2),mfrow=c(length(tx_dat),1))
l<-lapply(tx_dat, function(dat){ 
  dat$rounds<-as.numeric(gsub("Round_(\\d+)_Replicate_(\\d+)","\\1",dat$Replicate))
  plot(dat$rounds,dat$log.likelihood,xlab="Round",axes=FALSE,ylab="Log likelihood")
  axis(1,seq(1,4))
  axis(2,las=1)
})
```

Yes they are!


Ok, but what about convergence? Are the replicates (Numbers 1-5) converging on likelihoods and parameter values?

```{r}
library(scales)
library(sm)
models<-unique(tx.opts$Model)
par(mfrow=c(3,3))
l<-lapply(models,function(mod){
  files<-list.files(path="dadi_analysis/TX2D",pattern = paste(".*V.*\\.",mod,".optimized.*",sep=""),full.names = TRUE)
  all_reps<-do.call(rbind,lapply(files, function(file){
    dat<-parse_dadi_opt(file)
    dat$rep<-gsub(paste0("dadi_analysis/TX2D/(.*).",mod,".optimized.txt"),"\\1",file)
    return(dat)
  }))
  all_reps$rep<-as.numeric(as.factor(all_reps$rep))
 # lines_per_rep<-nrow(all_reps)/length(files)
  #all_reps$rep<-rep(1:length(files),each=lines_per_rep)
  all_reps$log.likelihood<-as.numeric(as.character(all_reps$log.likelihood))
  
  #get the parameter values
  params<-do.call(rbind,lapply(all_reps$optimized_params,function(x){
    p<-as.numeric(as.character(rbind(unlist(strsplit(x,",")))))
    return(p)
  }))
  colnames(params)<-unlist(strsplit(unlist(strsplit(all_reps$params[1],",")),"\\."))
  all_reps<-cbind(all_reps,params)
  
  #plot
  par(xpd=TRUE,mar=c(2,6,2,2))
  plot(0:(ncol(params)*2),xlim=c(0,2*(ncol(params))),ylim=c(min(all_reps$log.likelihood),max(all_reps[,colnames(params)])),bty="L",axes=FALSE,ann=FALSE,type='n')
  gwsca.vioplot(all_reps$log.likelihood,col="grey",add=TRUE,xpd=TRUE,at=0)
  points(jitter(rep(0,nrow(all_reps)),2.5),all_reps$log.likelihood,col=alpha(all_reps$rep,0.5),pch=19)
  axis(2,las=1,pos=-0.5)
  axis(1,lwd=0,label="log likelihood",at=0)
  #mapply(function(param,counts) browser(),colnames(params),2:(ncol(params)+1))
  p<-mapply(function(param,counts){
    par(new=TRUE)
    plot(0:(ncol(params)*2),xlim=c(0,2*(ncol(params))),ylim=c(min(all_reps[,param]),max(all_reps[,param])),bty="L",axes=FALSE,ann=FALSE,type='n')
    gwsca.vioplot(all_reps[,param],col="grey",add=TRUE,xpd=TRUE,ylim=c(min(all_reps[,param]),max(all_reps[,param])),at=counts)
    points(jitter(rep(counts,nrow(all_reps)),2.5),all_reps[,param],col=alpha(all_reps$rep,0.5),pch=19)
    axis(2,las=1,pos=counts-0.75)
    axis(1,lwd=0,label=param,at=counts)
  },colnames(params),seq(2,(ncol(params)*2),by=2))
  mtext(mod,3)
  
})
```

```{r}
tx.best[which.min(tx.best$AIC),]
```

So now I can run some simulations in dadi using the dadi-pipeline.

### Analyze Texas simulations

```{r}
sim_dirs<-grep("sims",list.dirs(path="dadi_analysis/TX2D",full.names = TRUE),value = TRUE)
sims<-do.call(rbind,lapply(sim_dirs,function(x) {
  
  opt_files<-list.files(pattern="Simulation.*optimized.txt",path=x,full.names = TRUE)
  
  dat<-do.call(rbind,lapply(opt_files,function(file){
   print(file)
    dat<-read.delim(file,header=TRUE)

    best<-dat[which.max(dat$log.likelihood),]
    return(best)
  }))
  params<-do.call(rbind,strsplit(as.character(dat$optimized_params),","))
  params<-apply(params,2,as.numeric)
  dat<-cbind(dat,params)
  return(dat)
}))
emps<-do.call(rbind,lapply(sim_dirs,function(x) {

  dat<-read.delim(paste0(x,"/Empirical.sym_mig_size.optimized.txt"),header=TRUE)
  dat$sim<-x
  return(dat)
}))

start_params<-c(nu1a=5.65, nu2a=4.46, nu1b=1.01, nu2b=19.94, m=0.46, T1=1.34, T2=0.40)
summary(sims$log.likelihood)
summary(sims$chi.squared)
```

Use distributions of log-likelihood and $\chi^2$ values from simulated datasets to evaluate whether the empircally-estimated values are contained within these distributions (following https://onlinelibrary.wiley.com/doi/full/10.1111/mec.14862). 

```{r}
### FROM dadi_pipeline's Plot_GOF.R
#####################################
#likelihood plot
ll_seq<- seq(-1500,-900,20)
hist(sims$log.likelihood, breaks=ll_seq, main = "Simulation Results - Log-likelihood distribution", xlab="log-likelihood", col="grey")
abline(v=emps$log.likelihood, lwd = 3, col='blue')

#chi-squared plot
chi_seq<- seq(1000,7000,50)
hist(sims$chi.squared, breaks=chi_seq, main = "Simulation Results - Chi-squared distribution ", xlab="Chi-squared test statistic", col="grey")
abline(v=emps$chi.squared, lwd = 3, col='blue')

#####################################
#For log-transformed chi.squared

#transform chi-squared test stat
log_chi <- log(sims$chi.squared)
emp_log_chi <- log(emps$chi.squared)
summary(log_chi)


#log transformed chi-squared plot
lchi_seq<- seq(7,9,0.05)
hist(log_chi, breaks=lchi_seq, main = "Simulation Results - Log Chi-squared distribution ", xlab="log Chi-squared test statistic", col="grey")
abline(v=emp_log_chi, lwd = 3, col='blue')


```

Now let's check the parameter estimates
```{r}
par(mfrow=c(3,3),oma=c(2,2,2,2),mar=c(2,2,2,2))

for(i in 1:length(start_params)){
  hist(sims[,7+i], breaks=50, main = names(start_params)[i], xlab=names(start_params)[i], col="grey")
  abline(v=start_params[i], lwd = 3, col='blue')
}

```


But I would like to possibly model varying rates across the genome, and at the very least use the simulations to help ID outliers. So I'm going to run ms and possibly design some additional dadi models with heterogeneous migration across the genome. 



## Troubleshooting warnings

This doesn't tell us about which models had trouble converging, though, so let's tally up the number of 'nan's in each file.

```{r}
tx.nans<-do.call(rbind,lapply(list.files(path = "dadi_analysis/TX2D",pattern="V2.*optimized.*",full.names = TRUE),dadi.nan))
```

Still not useful, let's extract warning messages from the log file!

```{r}
tx.warnings<-dadi_warnings("dadi_analysis/251_2DTXb.log")
tx.warnings$Warning<-TRUE
tx.opts<-do.call(rbind,lapply(list.files(path = "dadi_analysis/TX2D",pattern="V2.*optimized.*",full.names = TRUE),parse_dadi_opt))
tx.dat<-merge(tx.opts,tx.warnings,by=c("Model","Replicate"),all = TRUE)

```

Do I get vastly different parameter estimates for the ones with warnings?


```{r}
tx_dat<-extract_dadi_params(tx.dat)
par(xpd=TRUE,mar=c(2,6,2,2),mfrow=c(length(tx_dat),2))
pl<-lapply(tx_dat,function(dat){ 
  require(scales)
  require(sm)
  params<-colnames(dat)[(which(colnames(dat)=="Warning")+1):ncol(dat)]
  plot_params(dat[!is.na(dat$Warning),],params,pt.col = "coral")
  legend("topleft",paste("N=",nrow(dat[!is.na(dat$Warning),])),bty='n')
  plot_params(dat[is.na(dat$Warning),],params,pt.col = "grey")
  legend("topleft",paste("N=",nrow(dat[is.na(dat$Warning),])),bty='n')
})


```

What's the best model if I remove ones with warnings?

```{r}
tx.bests<-lapply(tx_dat,function(dat){ 
  keep<-dat[is.na(dat$Warning),]
  best<-keep[which.max(keep$log.likelihood),]
})
tx.bests[[which.min(unlist(lapply(tx.bests,function(x){ return(x$AIC) })))]]
```

Did a second round with better parameter inputs reduce the warnings?

```{r}
tx.warnings2<-dadi_warnings("dadi_analysis/251_2DTX_V2_2.log")
tx.warnings2$Warning<-TRUE
tx.opts2<-do.call(rbind,lapply(list.files(path = "dadi_analysis/TX2D",pattern="V2.*optimized.*",full.names = TRUE),parse_dadi_opt))
tx.dat2<-merge(tx.opts2,tx.warnings2,by=c("Model","Replicate"),all = TRUE)

```

Nope! The first one had `r nrow(tx.warnings[tx.warnings$Warning=="TRUE",])` and the second had `r nrow(tx.warnings2[tx.warnings2$Warning=="TRUE",])` warnings. I think it might be time to try the other type of model. 

I changed the optimization from `make_extrap_log_func()` to `make_extrap_func()`, which was suggested on the dadi forums.
```{r}
tx.warnings1<-dadi_warnings("dadi_analysis/251_2DTX_V1_1.log")
tx.warnings1$Warning<-TRUE
tx.opts1<-do.call(rbind,lapply(list.files(path = "dadi_analysis/TX2D",pattern="V1.*optimized.*",full.names = TRUE),parse_dadi_opt))
tx.dat1<-merge(tx.opts1,tx.warnings1,by=c("Model","Replicate"),all = TRUE)

```
But this didn't reduce the number of runs with warnings! There were `r dim(tx.warnings1[tx.warnings1$Warning=="TRUE",])` warnings in this run.

```{r}
tapply(tx.warnings1$Warning,tx.warnings1$Model,table)
tapply(tx.warnings2$Warning,tx.warnings2$Model,table)
tapply(tx.warnings$Warning,tx.warnings$Model,table)
```

What about the final set?

```{r}
tx.warnings3<-dadi_warnings("dadi_analysis/251_2DTX_V1_2.log")
tx.warnings3$Warning<-TRUE
tx.opts3<-do.call(rbind,lapply(list.files(path = "dadi_analysis/TX2D",pattern="V1_Number_2.*optimized.*",full.names = TRUE),parse_dadi_opt))
tx.dat3<-merge(tx.opts3,tx.warnings3,by=c("Model","Replicate"),all = TRUE)

```

```{r}
tx.warnings4<-dadi_warnings("dadi_analysis/251_2DTX_V1_3.log")
tx.warnings4$Warning<-TRUE
tx.opts4<-do.call(rbind,lapply(list.files(path = "dadi_analysis/TX2D",pattern="V1_Number_3.*optimized.*",full.names = TRUE),parse_dadi_opt))
tx.dat4<-merge(tx.opts4,tx.warnings4,by=c("Model","Replicate"),all = TRUE)
```
Let's look at it over replicates etc.
```{r}
tx.warnings4$Round<-as.numeric(gsub("Round_(\\d+)_Replicate_(\\d+)","\\1",tx.warnings4$Replicate))
tx.warnings4$Replicate<-as.numeric(gsub("Round_(\\d+)_Replicate_(\\d+)","\\2",tx.warnings4$Replicate))

cols<-c('#7fc97f','#beaed4','#fdc086','#ffff99','#386cb0','#f0027f','#bf5b17','#666666')
par(mfrow=c(2,4))
for(i in 1:length(unique(tx.warnings4$Model))){
  plot(tx.warnings4$Replicate[tx.warnings4$Model%in%unique(tx.warnings4$Model)[i]],
       tx.warnings4$Round[tx.warnings4$Model%in%unique(tx.warnings4$Model)[i]],
       col=cols[unique(tx.warnings4$Model)[i]],pch=19,xlab="Replicate",ylab="Round",main = unique(tx.warnings4$Model)[i],bty="L")
}

```


```{r}
tapply(tx.warnings1$Warning,tx.warnings1$Model,table)
tapply(tx.warnings2$Warning,tx.warnings2$Model,table)
tapply(tx.warnings$Warning,tx.warnings$Model,table)
tapply(tx.warnings3$Warning,tx.warnings3$Model,table)
tapply(tx.warnings4$Warning,tx.warnings4$Model,table)
```

Unlike the Florida runs, no one model seems to be disproportionately receiving error messages. I should also note that the TX one takes longer to run. Perhaps some of the parameters are simply in a more challenging part of parameter space? To evaluate this, let's look at the best parameters so far

```{r}
tx_dat<-extract_dadi_params(rbind(tx.dat,tx.dat2,tx.dat1,tx.dat3))
par(xpd=TRUE,mar=c(2,6,2,2),mfrow=c(length(tx_dat),2))
pl<-lapply(tx_dat,function(dat){ 
  require(scales)
  require(sm)
  params<-colnames(dat)[(which(colnames(dat)=="Warning")+1):ncol(dat)]
  plot_params(dat[!is.na(dat$Warning),],params,pt.col = "coral")
  legend("topleft",paste("N=",nrow(dat[!is.na(dat$Warning),])),bty='n')
  plot_params(dat[is.na(dat$Warning),],params,pt.col = "grey")
  legend("topleft",paste("N=",nrow(dat[is.na(dat$Warning),])),bty='n')
})


```

The ones with warnings are pretty bad and the estimates of nu are all over the place. What's the best model if I remove ones with warnings?

```{r}
tx.bests<-lapply(tx_dat,function(dat){ 
  keep<-dat[is.na(dat$Warning),]
  best<-keep[which.max(keep$log.likelihood),]
})
tx.bests[[which.min(unlist(lapply(tx.bests,function(x){ return(x$AIC) })))]]
```

nu2 estimates seem to be hitting the upper limit that I've set (15), so perhaps that's part of the issue. I'll run another round, increasing the upper limits, increasing the number of points, and updating the starting parameters.





```{r}
all_warnings<-lapply(list.files(pattern="251",path = "dadi_analysis",full.names = TRUE),dadi_warnings)
names(all_warnings)<-list.files(pattern="251",path = "dadi_analysis",full.names = FALSE)

#for now, focus on V1s
all_warnings<-all_warnings[c("251_2DTX_V1_1.log","251_2DTX_V1_2.log","251_2DTX_V1_3.log")] 
all_warnings<-lapply(all_warnings,function(dat) { 
  dat$Warning<-TRUE
  return(dat)
})
```

Now let's get the parameters
```{r}
opt_files<-list.files(path = "dadi_analysis/TX2D",pattern="V.*optimized.*",full.names = TRUE)
opt_files<-opt_files[grep("V1",opt_files)] #focus on V1s
tx_opts<-do.call(rbind,lapply(opt_files,function(file){
  dat<-parse_dadi_opt(file)
  dat$file<-file
  return(dat)
}))
```

```{r}
opt_warns<-do.call(rbind,mapply(function(warnings,name,opts){
  key<-gsub("251_2DTX_(V\\d)_(\\d).log","\\1_Number_\\2",name)
  dat<-merge(opts[grep(key,opts$file),],warnings,by=c("Model","Replicate"),all = TRUE)
  dat$Warning[is.na(dat$Warning)]<-FALSE
  return(dat)
},all_warnings,names(all_warnings),MoreArgs = list(opts=tx_opts),SIMPLIFY = FALSE))
```

Now let's go model by model and identify parameter distributions that cause warnings and ones that don't.

```{r}
library(scales)
library(sm)
par(mfrow=c(length(unique(opt_warns$Model)),2),mar=c(2,2,2,2))
model_params<-by(opt_warns,INDICES=factor(opt_warns$Model),function(x){
  warn_params<-data.frame(extract_dadi_params(x[x$Warning=="TRUE",]))
  ok_params<-data.frame(extract_dadi_params(x[x$Warning=="FALSE",]))
  warn_params[c("log.likelihood","AIC","chi.squared","theta")]<-apply(warn_params[c("log.likelihood","AIC","chi.squared","theta")],2,as.numeric)
  ok_params[c("log.likelihood","AIC","chi.squared","theta")]<-apply(ok_params[c("log.likelihood","AIC","chi.squared","theta")],2,as.numeric)
  params<-colnames(warn_params)[(which(colnames(x)=="Warning")+1):ncol(warn_params)]
  plot_params(warn_params,params,pt.col = "coral")
  legend("topleft",paste("N=",nrow(warn_params)),bty='n')
  plot_params(ok_params,params,pt.col = "grey")
  legend("topleft",paste("N=",nrow(ok_params)),bty='n')
  return(rbind(warn_params,ok_params))
})
```

```{r}
diffs<-lapply(model_params,function(dat){
  params<-colnames(dat)[(which(colnames(dat)=="Warning")+1):ncol(dat)]
  tests<-lapply(params,function(param,dat){
    res<-wilcox.test(dat[dat$Warning=="TRUE",param],dat[dat$Warning=="FALSE",param])  
    if(res$p.value<0.05) print(paste(param,"is different in",dat$Model[1]))
    return(res)
  },dat=dat)
  names(tests)<-params
  return(tests)
})

```

OK, how can I deal with this? 


## Other 2D models

How do I analyze the 2D models? I've got FLTX, FLAL, and ALTX to analyse.

```{r}

fltx.opts<-dadi.modelcomp(path = "dadi_analysis/FLTX",pattern=".*optimized.*",id="FLTX")
kable(fltx.opts,caption = "'Best' runs of each of the models for the FL vs TX 2D demographic inference")
```

```{r}

flal.opts<-dadi.modelcomp(path = "dadi_analysis/FLAL",pattern=".*optimized.*",id="FLAL")
kable(flal.opts,caption = "'Best' runs of each of the models for the FL vs AL 2D demographic inference")
```

```{r}

altx.opts<-dadi.modelcomp(path = "dadi_analysis/ALTX",pattern=".*optimized.*",id="ALTX")
kable(altx.opts,caption = "'Best' runs of each of the models for the AL vs TX 2D demographic inference")
```
