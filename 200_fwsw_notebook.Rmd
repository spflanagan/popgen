---
title: "FWSW lab Notebook"
author: "Sarah P. Flanagan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup}
knitr::opts_knit$set(root.dir='../fwsw_results/', fig.pos='H')
```

# 10 October 2019

Bayenv is done, now let's see what the results are.

```{r}

get_bayenv_results<-function(dir,env_vars){
  # process the variable names
  var_names<-unlist(lapply(env_vars,function(var){
    nms<-c(paste0(var,"_BF"),paste0(var,"_rho"),paste0(var,"_rs"))
    return(nms)
  }))
  # list all the files
  bf.files<-list.files(pattern="bf",path = dir,full.names = TRUE)
  xtx.files<-list.files(pattern="xtx",path=dir,full.names = TRUE)
  # get the bf files
  bf.dat<-do.call(rbind,lapply(bf.files,function(filename){
    bf<-read.table(filename,header = FALSE)
  }))
  colnames(bf.dat)<-c("locus", var_names)
  
  # xtx files
  xtx.files<-list.files(pattern="xtx",path=dir,full.names = TRUE)
  xtx.dat<-do.call(rbind,lapply(xtx.files,function(filename){
    xtx<-read.table(filename,header = FALSE,stringsAsFactors = FALSE)
  }))
  colnames(xtx.dat)<-c("locus","XtX")
  
  # combine the two
  bayenv.dat<-merge(xtx.dat,bf.dat,by="locus")
  return(bayenv.dat)
}
get_bayenv_results(dir="bayenv/SNPFILES",env_vars=c("temp","salinity","seagrass"))


```

The SNP names are uninformative, just the row number the SNP was in. We can make these better using the freq info

```{r}
freq<-read.table("bayenv/bayenv.frq.strat",header=T, stringsAsFactors=F)
#want to get $MAC for every snp at every pop 
#and NCHROBS-MAC for every stnp at every pop
freq<-cbind(freq,freq$NCHROBS-freq$MAC)
colnames(freq)[ncol(freq)]<-"NAC"
pop.order<-levels(as.factor(freq$CLST))
snp.names<-split(freq$SNP,freq$CLST)[[1]]
snp.names<-gsub("(\\d+)_\\d+","\\1",snp.names)

loc<-as.numeric(gsub("SNPFILES/(\\d+)","\\1",bayenv.dat$locus))

bayenv.dat$locus<-snp.names[loc]
write.table(bayenv.dat,"bayenv/bayenv_output.txt",sep="\t",col.names = TRUE,row.names = FALSE,quote = FALSE)
```

Ok, I've added this to the 200_fwsw_reanalysis.Rmd document. Next step will be to plot it, which will require chromosome and bp info -- I could get this from the map file I suppose, or the vcf.




# 9 October 2019

I started a populations run at home requiring loci to be in all populations, we'll see if that runs or not.

It looks like the initial runs of the FLAL, FLTX, and ALTX combinations ran without any problems, so I'll run another set, especially since the simulations are still running. I wonder if I should be doing all the pairwise comparisons. I should at least do LA compared to the others -- I've written that but will hold off running them until the simulations are done and I can re-start my computer.

Now I'm going to focus back on bayenv analysis - I've successfully run bayenv, it seems, and I've got xtx and bf files in the SNPFILES/ directory. Now I need to aggregate them. Looking at code from the popgen paper, I can see that yes I've done this before. Now I'll just write a couple functions to do this. 

```{r}
dir="bayenv/SNPFILES"
env_vars<-c("temp","salinity","seagrass")
var_names<-unlist(lapply(env_vars,function(var){
  nms<-c(paste0(var,"_BF"),paste0(var,"_rho"),paste0(var,"_BF"))
  return(nms)
}))
bf.files<-list.files(pattern="bf",path = dir,full.names = TRUE)
xtx.files<-list.files(pattern="xtx",path=dir,full.names = TRUE)

bf.dat<-do.call(rbind,lapply(bf.files,function(filename){
  bf<-read.table(filename,header = FALSE)
}))

colnames(bf.dat)<-c("locus", var_names)
```

Ok, I'm running into an interesting issue, which is that some of the files only have one number instead of three for each variable. The only problematic one seems to be 1776.freq.bf, as well as its neightbor 1776.freq.bf.freqs 

```{r}
bf.dat<-lapply(bf.files,function(filename){
   bf<-read.table(filename,header = FALSE)
  # if(ncol(bf)<10){ browser() }
 })
ncols<-lapply(bf.dat,ncol)
which(ncols<10)

bf.dat[which(ncols<10)]
bf.files[which(ncols<10)]
```


this gives me an indication that possibly bayenv ran on 1776.freq.bf somehow and now has weird stuff going on. I think I need to re-run bayenv on it and delete the other files. 

I re-ran that analysis using `~/Programs/bayenv/bayenv2 -i 1776 -m ../matrix -e ../env_data_sub75.txt -p 7 -k 100000 -n 3 -t -c -f -X -r 6842483 -o 1776.freq` and got this output

```
===== BAYENV2.0 =====

input file is set
matrix is set
environment file is set
number of populations is set
number of iterations is set
number of environmental variables is set
TEST is set
correlation output is set
frequency output is set
Calculating XtX
seed is set
output file is set
TEST = 1 . So running test at a SNP
number of environmental variables 3

MCMC VER 0.71 (THREADED)
ITERATIONS = 100000
INPUT FILE = 1776
MATRIX FILE = ../matrix
SEED = -6842483

ENVIRON FILE = ../env_data_sub75.txt
reading environ
num_alleles = 2.000000
number of loci = 1
```

So now let's try the code above again...if it works, I can continue working on it. 

```{r}
dir="bayenv/SNPFILES"
env_vars<-c("temp","salinity","seagrass")
var_names<-unlist(lapply(env_vars,function(var){
  nms<-c(paste0(var,"_BF"),paste0(var,"_rho"),paste0(var,"_BF"))
  return(nms)
}))

bf.files<-list.files(pattern="bf",path = dir,full.names = TRUE)
bf.dat<-do.call(rbind,lapply(bf.files,function(filename){
  bf<-read.table(filename,header = FALSE,stringsAsFactors = FALSE)
}))

colnames(bf.dat)<-c("locus", var_names)

xtx.files<-list.files(pattern="xtx",path=dir,full.names = TRUE)
xtx.dat<-do.call(rbind,lapply(xtx.files,function(filename){
  xtx<-read.table(filename,header = FALSE,stringsAsFactors = FALSE)
}))
colnames(xtx.dat)<-c("locus","XtX")

bayenv.dat<-merge(xtx.dat,bf.dat,by="locus")


```

Let's get the SNP names to put in the bayenv dataframe...

```{r}
freq<-read.table("bayenv/bayenv.frq.strat",header=T, stringsAsFactors=F)
#want to get $MAC for every snp at every pop 
#and NCHROBS-MAC for every stnp at every pop
freq<-cbind(freq,freq$NCHROBS-freq$MAC)
colnames(freq)[ncol(freq)]<-"NAC"
pop.order<-levels(as.factor(freq$CLST))
snp.names<-split(freq$SNP,freq$CLST)[[1]]
snp.names<-gsub("(\\d+)_\\d+","\\1",snp.names)

bayenv.dat$locus<-snp.names
```

Huh, this highlights an interesting issue, that I somehow have 24205 bf files but should only have 12103 loci. I wonder if perhaps my bayenv thing wasn't set up properly...So the bayenv file should have two rows per locus. 

I think the issue is how I created the SNPFILES -- I will need to re-do this. I think I've fixed it, so now it is 
```{r}
for(i in seq(1,(nrow(snpsfile)-1),2)){
  write.table(snpsfile[i:(i+1),],paste0(directory,"/locus",rownames(snpsfile)[i]),
              quote=F,col.names=F,row.names=F,sep='\t')
}
```

Before running this I'm going to delete the old SNPSfiles and bayenv stuff. Ok, this works! `../../scripts/run_bayenv2_matrix_general.sh SNPFILES SNPSFILE SNPFILES `.

Now to re-run bayenv...
```{bash}
nohup ../../scripts/run_bayenv2_matrix_general.sh BAYENV ~/Programs/bayenv/ matrix env_data_sub75.txt 7 3 SNPFILES > bayenv.log &
```

Ok, it's running! 

# 7 October 2019

The dadi TX simulations are almost done, but it turns out I haven't run the analyses with heterogeneous genomic migration rates. I also haven't run all of the other comparisons - maybe that's what I should run next. So I started the FLFW vs TXFW, FLFW vs ALFW, and ALFW vs TXFW. So now I've got 7 dadi runs going at once, and I'll start more eventually. 

When I get back, these are the priorities:
* Re-run populations to include all 16 populations but require more populations to be present.
* Re-run structure and treemix
* Re-run adegenet, PCAdapt, etc.


# 30 September 2019

On Saturday I wrote an R script (`plot_bayenv_matrices.R`) to plot the final matrix from each bayenv run. Now I'm going to update that script to randomly choose one and write it to `matrix` for the next bayenv step.

Ok, I've created the SNPFILEs but something is up with the environmental file, it looks like. I need to reformat it!

```{r}
env_dat<-read.csv("bayenv/env_data_raw.csv",row.names = 1)
std_dat<-t(apply(env_dat,1,function(x){
  stands<-(x-mean(x))/sd(x)
  return(stands)
}))
# make sure it's in the same order as the plinkfile
freq<-read.table("bayenv/bayenv.frq.strat",header=T, stringsAsFactors=F)
freq<-cbind(freq,freq$NCHROBS-freq$MAC)
colnames(freq)[ncol(freq)]<-"NAC"
pop.order<-levels(as.factor(freq$CLST))
snp.names<-split(freq$SNP,freq$CLST)[[1]]


write.table(std_dat[,pop.order],
            "bayenv/env_data_sub75.txt",sep='\t',col.names = FALSE,row.names = FALSE)
```


# 27 September 2019

`plink --file data --extract mysnps.txt`

SNP names are different, though, in the map than in the vcf, so I've gotta convert them. 

```{r}
map<-read.table("stacks/populations_subset75/batch_2.plink.map",header=F)	
vcf<-parse.vcf("stacks/populations_subset75/batch_2.pruned.vcf")
map$id<-gsub("(\\d+)_\\d+","\\1",map$V2)
map$pos<-map$V4+1

loci<-map[which(map$id %in% vcf$ID & map$pos %in% vcf$POS),]
```

This has too many rows, which means that some of them are duplicated. Let's dig into this...

```{r}
dups<-loci[loci$id %in% loci$id[duplicated(loci$id)],]
head(dups)
vcf[vcf$ID==18109,1:3]
```

What this shows me is that the double %in% doesn't work right. 

Ok, let's do this instead

```{r}
loci<-map[paste(map$id,map$pos,sep="_") %in% paste(vcf$ID,vcf$POS,sep="_"),]
```

That works! Now we need to write the list of snps to a file.

```{r}
write.table(loci$V2,"keptSNPs_plink.txt",quote=FALSE,col.names = FALSE,row.names = FALSE)
```

so then I ran ` ~/Programs/plink-1.07-x86_64/plink --file stacks/populations_subset75/batch_2.plink --extract keptSNPs_plink.txt --noweb --recode --out stacks/populations_subset75/batch_2.pruned`

And it seems to have worked...

```{r}
ped<-read.delim("stacks/populations_subset75/batch_2.pruned.ped",sep=' ',header = FALSE)
map<-read.delim("stacks/populations_subset75/batch_2.pruned.map",header = FALSE)
```

Except it seems to have removed the LG info from the map file. I can probably reconstruct this, though...

```{r}
map$V1<-vcf$`#CHROM`[paste(vcf$ID,vcf$POS,sep="_") %in% paste(gsub("(\\d+)_\\d+","\\1",map$V2),map$V4+1,sep="_")]
write.table(map,"stacks/populations_subset75/batch_2.pruned.map",col.names = FALSE,row.names = FALSE,quote=FALSE,sep='\t')
```

Ok, awesome, now I can run bayenv.

WAIT! these are runs with only the fw-sw pairs...do I want to use the whole populations set? If so, then maybe I don't have to run this again, but the loci probably won't match up. 

If I do want to run it again, what do I need to do to specify populations? Put the Pop ID in the family line? Ah, no, with a cluster file (i.e., a pop map)

```{r}
clust<-ped[,1:2]
clust$clust<-gsub("sample_(\\w{4}).*","\\1",clust$V2)
write.table(clust,"bayenv/sub75.pruned.clust",col.names = FALSE,row.names = FALSE,quote=FALSE)
```

Ok, I ran 
```
../scripts/run_bayenv2_matrix_general.sh bayenv/sub75.pruned.clust stacks/populations_subset75/batch_2.pruned.ped stacks/populations_subset75/batch_2.pruned.map bayenv ~/Programs/bayenv/
```
But this doesn't quite work, it says 
```
===== BAYENV2.0 =====

input file is set
number of populations is set
number of iterations is set
seed is set

!!!Missing or wrong options for chosen configuration.
```

This has to do with the script I'm using and the fact that it's not quite right and kind of janky.

Also, the plink command in the script is not creating the SNPSFILE required by bayenv...Looking back at my notes from the popgen paper, I converted the plink output into bayenv SNPSFILE in R.

I found the code and created a new script that I can run, SNPSFILEfromPLINKfrq.R, and that seems to work. 

So now I'm generating the matrices. 

What else? I really should figure out some way to plot the outliers but I'm feeling really lazy. 

# 26 September 2019

So I'm a bit concerned that I've been using the wrong vcf file to generate the dadi analyses and to do these outlier analyses. This is not a HUGE deal for the outliers, since I can fairly easily re-run them, but for dadi this is a bit concerning. I *think* I should be using `populations_subset75/batch_2.pruned.vcf`, but it looks like I used `populations_subset75/batch_2.vcf` instead. Of course, I didn't do a good job recording WHAT I pruned it for. 

Let's compare them.

```{r}
vcf<-parse.vcf("stacks/populations_subset75/batch_2.vcf")
pruned_vcf<-parse.vcf("stacks/populations_subset75/batch_2.pruned.vcf")
```

batch_2.vcf has 62497 loci, and pruned_vcf has 12103. Ok, looking more carefully at my dadi code, I see that I did subset them - whew. It was in making the dadi stuff that I created the pruned vcf. Yay former me! 

Ok, let's work on subsetting the ped and map files then. Have I solved this problem before, and would the code be in gwscaR? It doesn't look like it. But plink probably does this? Yes, it does, but I have to figure out how again. 

`plink --file data --extract mysnps.txt`



# 25 September 2019

Returning to my goal of having a 'master' data frame summarising my SNPs, I need to figure out the conversion between the stacks fst BP output and the vcf POS output. Why can't they all just be the same?!?!

```{r stacks_fsts}
fw_SNPinfo<-readRDS("fw_SNPinfo.RDS")
fwsw.al<-read.delim("stacks/populations_subset75/batch_2.fst_ALFW-ALST.tsv")
fwsw.la<-read.delim("stacks/populations_subset75/batch_2.fst_ALST-LAFW.tsv")
fwsw.tx<-read.delim("stacks/populations_subset75/batch_2.fst_TXCC-TXFW.tsv")
fwsw.fl<-read.delim("stacks/populations_subset75/batch_2.fst_FLCC-FLFW.tsv")
```

And we need to match these to the Stacks Fsts, which have multiple SNPs per locus. And of course the BP and Position don't match up between the two -- I **thought** it was BP = POS+1, but that doesn't seem to work in all cases. And of course the stacks manual is useless. The google groups says that "Stacks starts counting at 0" [unlike SAM files apparently] but this doesn't clarify the vcf vs fst output.  

```{r}
test<-merge(fw_SNPinfo,fwsw.al,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all = FALSE) 
```

This test only has 6918 rows and it should have 12103. If we keep all of the fw_SNPinfo ones:

```{r}
test<-merge(fw_SNPinfo,fwsw.al,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all.x=TRUE,all.y = FALSE) 
head(test)
```

We can see that this is super inconsistent and confusing. Let's look at the first NA one.

```{r}
fwsw.al[which(fwsw.al$Locus.ID == fw_SNPinfo$ID[3]),]
fw_SNPinfo[3,]
```

The locus is found in both but the BP is not! I wonder if some SNPs were removed from the Fst comparisons due to coverage issues or something. 

```{r}
testa<-merge(fw_SNPinfo,fwsw.al,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all = FALSE) 
testl<-merge(fw_SNPinfo,fwsw.la,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all = FALSE) 
testt<-merge(fw_SNPinfo,fwsw.tx,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all = FALSE) 
testf<-merge(fw_SNPinfo,fwsw.fl,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all = FALSE) 
```

They've all got different numbers of rows - AL has 6918, LA has 7042, TX has 3687, and FL has 2049. UGH how annoying. Ok, well we'll just move forward with this, I can't waste my life trying to make sense of this weirdness. I *could* re-run populations, but I'd need to do that at home, so I'll consider that later.

```{r}
fw_SNPinfo<-readRDS("fw_SNPinfo.RDS")
fw_SNPinfo<-merge(fw_SNPinfo,fwsw.al,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all.x=TRUE,all.y = FALSE)[,c(colnames(fw_SNPinfo),"Fisher.s.P")] 
colnames(fw_SNPinfo)[ncol(fw_SNPinfo)]<-"stacks_AL"
fw_SNPinfo<-merge(fw_SNPinfo,fwsw.la,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all.x=TRUE,all.y = FALSE)[,c(colnames(fw_SNPinfo),"Fisher.s.P")] 
colnames(fw_SNPinfo)[ncol(fw_SNPinfo)]<-"stacks_LA"
fw_SNPinfo<-merge(fw_SNPinfo,fwsw.tx,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all.x=TRUE,all.y = FALSE)[,c(colnames(fw_SNPinfo),"Fisher.s.P")] 
colnames(fw_SNPinfo)[ncol(fw_SNPinfo)]<-"stacks_TX"
fw_SNPinfo<-merge(fw_SNPinfo,fwsw.fl,by.x=c("Chrom","BP"),by.y=c("Chr","BP"),all.x=TRUE,all.y = FALSE)[,c(colnames(fw_SNPinfo),"Fisher.s.P")] 
colnames(fw_SNPinfo)[ncol(fw_SNPinfo)]<-"stacks_FL"
saveRDS(fw_SNPinfo,"fw_SNPinfo.RDS")
```

Ok, I've added that to the main analysis document (which, by the way, I think I probably need to split into sections because it's so long!). Anyway, now let's add on PCAdapt and Bayenv outliers. I'm going to use the existing code in the doc to walk through PCAdapt.


I should note that for some of these PCAdapt gives "NA" -- not sure what causes this behaviour but there it is. It's true for the residuals and everything else. It looks to be due to low allele frequencies -- though stacks should have been run with a minimum allele frequency cutoff, so this is perplexing.

### Bayenv?

Let's see what I did previously.

```{r compareEnvVar}
env.data<-read.csv("bayenv/env_data_raw.csv",row.names = 1)
env.data<-rbind(env.data,pop=c(rep("SW",12),rep("FW",4)))
env.data<-as.data.frame(t(env.data))
wilcox.test(as.numeric(env.data$temp)~env.data$pop) #ties, but p=0.539
wilcox.test(as.numeric(env.data$seagrass)~env.data$pop) #ties, but p=0.897
```


```{r bayenv}
#taken directly from fwsw_analysis.R
bf<-read.delim("bayenv/p4.bf.txt",header=T)
bf$SNP<-paste(bf$scaffold,as.numeric(as.character(bf$BP))+1,sep=".")
bf.co<-apply(bf[,5:7],2,quantile,0.99) #focus on Bayes Factors, because of Lotterhos & Whitlock (2015)
temp.bf.sig<-bf[bf$Temp_BF>bf.co["Temp_BF"],c(1,2,4,8,5,9)]
sal.bf.sig<-bf[bf$Salinity_BF>bf.co["Salinity_BF"],c(1,2,4,8,6,9)]
grass.bf.sig<-bf[bf$seagrass_BF>bf.co["seagrass_BF"],c(1,2,4,8,7,9)]
#get the log transformed Bayes Factors
bf$logSal<-log(bf$Salinity_BF)
bf$logTemp<-log(bf$Temp_BF)
bf$logSeagrass<-log(bf$seagrass_BF)

```

There are `r nrow(temp.bf.sig[temp.bf.sig$locus %in% sal.bf.sig$locus & temp.bf.sig$locus %in% grass.bf.sig,])` overlapping outliers between temperature-, salinity-, and seagrass-associated loci.

But if we only care about salinity ones, there are `r nrow(temp.bf.sig)` outliers.

This used a different dataset, so let's see if ANY of the loci are the same 

```{r}
fw_SNPinfo<-readRDS("fw_SNPinfo.RDS")
compare<-merge(fw_SNPinfo,bf,by.x = c("Chrom","BP"),by.y=c("scaffold","BP"))
```
... and `r nrow(compare)` are the same :cryface: 

So I think I need to re-run Bayenv. It looks like I already tried to do this, as I've got a fw_sub75.clust file. The run_bayenv2_matrix_general seems to be the one I want. 

It also looks like I need re-subset the ped and map files based on my vcf that I'm using.....and looking back at stuff, I think I'm currently using my dadi file, which maybe wasn't subsetted FML why is past me the worst???? Do I need to re-run dadi with a different dataset?!?!?!?!? goiajefpoiuh;oiaejoijevaoiejfoai 

# 20 September 2019

Apparently my computer restarted last night so I re-started the TX 2D simultaions. 

# 19 September 2019

1000 permutations took a while - finished ~20hrs later.

Now I'm going take a look at those that are never found in permuted range.

```{r}
perm_out<-lapply(permuted_fsts,function(x){
  outs<-x[x$act_in_perm==1,]
  return(outs)
})
names(perm_out)<-list("TXFW vs TXCC","FLFW vs FLCC","ALFW vs ALST","ALST vs LAFW")
```

```{r}
pf1a<-fst.plot(permuted_fsts[[1]],fst.name = "Fst",bp.name="Pos",pt.cex = 1,axis.size = 1,pch=19,y.lim = c(0,1))
points(pf1a$plot.pos[pf1a$act_in_perm==1],pf1a$Fst[pf1a$act_in_perm==1],col="red")
```

Hmm, the points are all over, even small values. Let's look at all four comparison.

```{r}
par(mfrow=c(4,1),mar=c(2,2,2,2))
lapply(permuted_fsts,function(perm_fst){
  pf1a<-fst.plot(perm_fst,fst.name = "Fst",bp.name="Pos",pt.cex = 1,axis.size = 1,pch=19,y.lim = c(0,1))
  points(pf1a$plot.pos[pf1a$act_in_perm==1],pf1a$Fst[pf1a$act_in_perm==1],col="red")
})
```

The nice thing is that most of the extreme outliers are captured in this analysis. The next thing to do would be to compare all of the outliers. I also should start some dadi runs.

I also started more TX simulations (4 sets of 20 sims). Hopefully none of them take 20 days! It would be really great if I could start a bunch of runs on the supercomputer or with Felipe or something. 

Before I leave for the day, I'm going to start my 'master' list of SNPs and their outlier status info. The locus IDs are in the vcf, not in the permuted_fsts dataframes, so I'll start with that.

```{r}
vcf<-parse.vcf("stacks/populations_subset75/batch_2.pruned.vcf")
fw_SNPinfo<-data.frame(ID=vcf$ID,Chrom=vcf$`#CHROM`,Pos=vcf$POS,
                       REF=vcf$REF,ALT=vcf$ALT,BP=vcf$POS-1,
                       perm_TX=permuted_fsts[[1]]$act_in_perm,
                       perm_FL=permuted_fsts[[2]]$act_in_perm,
                       perm_AL=permuted_fsts[[3]]$act_in_perm,
                       perm_LA=permuted_fsts[[4]]$act_in_perm,
                       stringsAsFactors = FALSE)
saveRDS(fw_SNPinfo,"fw_SNPinfo.RDS")
```


```{r stacks_fsts}
fwsw.al<-read.delim("stacks/populations_subset75/batch_2.fst_ALFW-ALST.tsv")
fwsw.la<-read.delim("stacks/populations_subset75/batch_2.fst_ALST-LAFW.tsv")
fwsw.tx<-read.delim("stacks/populations_subset75/batch_2.fst_TXCC-TXFW.tsv")
fwsw.fl<-read.delim("stacks/populations_subset75/batch_2.fst_FLCC-FLFW.tsv")
```

And we need to match these to the Stacks Fsts, which have multiple SNPs per locus. And of course the BP and Position don't match up between the two -- I **thought** it was BP = POS+1, but that doesn't seem to work in all cases. Ugh how frustrating. I really don't get it. 
```{r}
for(i in 1:nrow(fw_SNPinfo)){
  browser()
  fwsw.al$Fisher.s.P[which(fwsw.al$BP[which(fwsw.al$Locus.ID == fw_SNPinfo$ID[i])] == fw_SNPinfo$Pos[i]-1)]
}
test<-merge(fw_SNPinfo,fwsw.al,by.x=c("Chrom","BP"),by.y=c("Locus.ID","Chr","BP"),all.y = FALSE)
```

Hmm ok this is confusing and frustrating, I think I'm going to leave it at this and go home. 


# 18 September 2019

My goals for today are as follows:

1. Make overlapping histograms of Fst distributions for permuted and actual values. 
2. Use permutations to identify outliers (probability of actual value within permuted range?)
3. Spawn more dadi runs - possibly in parallel?
4. Create dataframe which contains for each locus:
* its information
* its stacks p-value
* its permutation probabilities
* its PCAdapt q-vale
* its XTX outlier designation
5. Make PCAdapt and Bayenv outlier plots 

So for #1 I'll start with the existing permutation distributions, just to write the code. 

```{r}
permuted_fsts<-readRDS("permuted_fsts.RDS")

```

```{r}
pop.list<-c("TXSP","TXCC","TXFW","TXCB","LAFW","ALST","ALFW","FLSG","FLKB",
	"FLFD","FLSI","FLAB","FLPB","FLHB","FLCC","FLLG")
pop.labs<-c("TXSP","TXCC","TXFW","TXCB","LAFW","ALST","ALFW","FLSG","FLKB",
            "FLFD","FLSI","FLAB","FLPB","FLHB","FLCC","FLFW")
fw.list<-c("TXFW","LAFW","ALFW","FLLG")
sw.list<-c("TXSP","TXCC","TXCB","ALST","FLSG","FLKB",
	"FLFD","FLSI","FLAB","FLPB","FLHB","FLCC")
all.colors<-c(rep("black",2),"#2166ac","black","#2166ac","black","#2166ac",
        rep("black",8),"#2166ac")
#grp.colors<-c('#e41a1c','#377eb8','#4daf4a','#984ea3','#ffff33','#f781bf')
grp.colors<-c('#762a83','#af8dc3','#e7d4e8','#d9f0d3','#7fbf7b','#1b7837')

# used this order for permutations
#pwise_maps<-list(popmap[popmap$pops %in% c("TXFW","TXCC"),],
#                 popmap[popmap$pops %in% c("FLLG","FLCC"),],
#                 popmap[popmap$pops %in% c("ALFW","ALST"),],
#                 popmap[popmap$pops %in% c("LAFW","ALST"),])

plot_labs<-list("TXFW vs TXCC","FLFW vs FLCC","ALFW vs ALST","ALST vs LAFW")
pt_cols<-list(TXTX=grp.colors[1],FLFL=grp.colors[6],
              ALAL=grp.colors[3],ALLA=grp.colors[2])
```

```{r}


plot_fst_hists<-function(perms,plot_lab=NULL,cols=NULL,permlab="mean_perm",reallab="Fst",baseplot=TRUE,inset=NULL){
  require(scales)
  if(is.null(plot_lab)){
    plot_lab<-""
  }
  if(is.null(cols)){
    cols<-c("grey","black")
  } else if(length(cols)==1){
    cols<-c("dark grey",cols)
  }
  #inset<-par()$fig
  #browser()
  if(isTRUE(baseplot)){
    hist(perms[,permlab],col=alpha(cols[1],0.5),border = alpha(cols[1],0.5),
         xlim=c(0,1),breaks = seq(0,1,0.01),main = plot_lab,xlab=expression(italic(F)[ST]),
         ylab="Number of SNPs")
    hist(perms[,reallab],col=alpha(cols[2],0.5),border = alpha(cols[2],0.5),
         xlim=c(0,1),breaks = seq(0,1,0.01),main = "",xlab=expression(italic(F)[ST]),
         ylab="Number of SNPs",add=TRUE)
  }
  if(!is.null(inset)){ # add an inset
    # adjust the fig coordinates
    ifig<-c(inset[1]+0.25*(inset[2]-inset[1]),inset[2], 
            inset[3]+0.25*(inset[4]-inset[3]), inset[4])
    par(fig = ifig,new=TRUE) # start x, end x, start y, end y (percent plotting space)
    hist(perms[,permlab][perms[,reallab]>0],col=alpha(cols[1],0.5),border = alpha(cols[1],0.5),
         xlim=c(0,1),breaks = seq(0,1,0.01),main = "",xlab="",
         ylab="")
    box() #give it a box
    hist(perms[,reallab,][perms[,reallab]>0],col=alpha(cols[2],0.5),border = alpha(cols[2],0.5),
         xlim=c(0,1),breaks = seq(0,1,0.01),main = "",xlab="",
         ylab="",add=TRUE)
  }
  invisible(par()$fig)
}

```
```{r}
png("../figs/permuted_fsts.png",pointsize = 16,height=7,width=8,units="in",res=300)
par(mfrow=c(2,2),new=FALSE,mar=c(4,4,3,1))
#plot the base
pars<-mapply(plot_fst_hists,perms=permuted_fsts,plot_lab=plot_labs,cols=pt_cols,SIMPLIFY = FALSE)
# add the insets
ipars<-mapply(plot_fst_hists,perms=permuted_fsts,plot_lab=plot_labs,cols=pt_cols,
              inset=pars,
              MoreArgs = list(baseplot=FALSE))
dev.off()
```

Ok, this works!


I need to change the output a bit and somehow evaluate the likelihood that the actual value is within the range of permuted values.
```{r}
permute.gwsca<-function(vcf,map1,nperms,z=1.96, maf.cutoff = 0.05,cov.thresh=0.2){
  # calculate the actuals
  actual_fsts<-gwsca(vcf,colnames(vcf)[1:9],
                     map1[map1[,2] %in% unique(map1[,2])[1],1],
                     map1[map1[,2] %in% unique(map1[,2])[2],1],
                     maf.cutoff=maf.cutoff,prop.ind.thresh=cov.thresh)
  # do the permutations
  perm_fsts<-lapply(1:nperms,function(i,vcf,map1){
    perm_map<-map1
    perm_map[,2]<-perm_map[,2][permute::shuffle(perm_map[,2])]
    perm_dat<-gwsca(vcf,colnames(vcf)[1:9],
                     perm_map[perm_map[,2] %in% unique(perm_map[,2])[1],1],
                     perm_map[perm_map[,2] %in% unique(perm_map[,2])[2],1],
                     maf.cutoff,cov.thresh)
   
    return(perm_dat)
  },vcf=vcf,map1=map1)
  
  # calculate stats
  fsts<-t(do.call(rbind,lapply(perm_fsts,'[[',"Fst"))) #extract permuted fsts
  perm_fst_mu<-rowMeans(fsts)
  perm_fst_in<-NULL
  for(i in 1:nrow(actual_fsts)){
    pmax<-max(fsts[i,] )
    pmin<-min(fsts[i,] )
    if(actual_fsts[i,"Fst"] > pmax | actual_fsts[i,"Fst"] < pmin ){
      perm_fst_in[i]<-1
    }else{
      perm_fst_in[i]<-0
    }
  }
  
  fst_dat<-data.frame(cbind(actual_fsts,
                            n_perms=nperms,
                            mean_perm=perm_fst_mu,
                            act_in_perm=perm_fst_in))
  return(fst_dat)
}
```
```{r}
vcf<-parse.vcf("stacks/populations_subset75/batch_2.pruned.vcf")
popmap<-data.frame(inds=colnames(vcf)[10:ncol(vcf)],
                   pops=gsub("sample_(\\w{4}).*","\\1",colnames(vcf)[10:ncol(vcf)]),
                   stringsAsFactors = FALSE)
pwise_maps<-list(popmap[popmap$pops %in% c("TXFW","TXCC"),],
                 popmap[popmap$pops %in% c("FLLG","FLCC"),],
                 popmap[popmap$pops %in% c("ALFW","ALST"),],
                 popmap[popmap$pops %in% c("LAFW","ALST"),])

TXmap<-popmap[popmap$pops %in% c("TXFW","TXCC"),]
perm_test<-permute.gwsca(vcf,TXmap,10,maf.cutoff=0,cov.thresh = 0)
```

Ok, this seems to have worked (it's a crude measure but let's go with it). Let's re-run this and generate many more permutations, then re-make the plots. 

```{r}
permuted_fsts<-lapply(pwise_maps,permute.gwsca,vcf=vcf,nperms=1000, maf.cutoff=0)
saveRDS(permuted_fsts,"permuted_fsts.RDS")
```
```{r}
png("../figs/permuted_fsts.png",pointsize = 16,height=7,width=8,units="in",res=300)
par(mfrow=c(2,2),new=FALSE,mar=c(4,4,3,1))
#plot the base
pars<-mapply(plot_fst_hists,perms=permuted_fsts,plot_lab=plot_labs,cols=pt_cols,SIMPLIFY = FALSE)
# add the insets
ipars<-mapply(plot_fst_hists,perms=permuted_fsts,plot_lab=plot_labs,cols=pt_cols,
              inset=pars,
              MoreArgs = list(baseplot=FALSE))
dev.off()
```



# 17 September 2019

My computer crashed -- 3DAL had been running for over 1600 hr and didn't even get through one replicate of one model. This is absolutely ridiculous. Luckily 254_2DoptNsim_TX_1 finished yesterday. Ah except it's not that it finished, it ran out of space! Crap. But it has finished 20 simulations so far. What's crazy to me is the variation in analysis times for the simulations -- from 5hr 49 min to 17 days! Once I restart my computer I'll run some more dadi combinations. maybe I can just do a bunch of 2D analyses and skip the 3D, unless Felipe sorts me out with something through MS Azure.

In the meantime, I'm going to work on an aspect of these analyses that is more within my control -- the Fst permutations.

```{r}
permuted_fsts<-readRDS("permuted_fsts.RDS")
```

So one of the issues with the permutations is that many of the loci are only polymorphic in one population, causing Fst to be 0, based on my calculations (because average Hs is the same as Ht). I could try another Fst calculation method but I'm not sure that will really help. But this should come out in the permutations? Are the permutations not permuting the map reliably? I think it should be. 

Looking at the variation in the Num1s after 10 permutations, it would seem that the permutations are working. What seems to be happening is that at some loci there is only one heterozygote so even if that individual gets switched it doesn't change things. 

I suppose the question is whether these perhaps-problematic loci are also problematic in the fst comparisons. The Fst output from permute.gwsca is the ACTUAL fst so I could plot those with CIs.

```{r gwscaPlot}
pf1<-fst.plot(permuted_fsts[[1]],bp.name="Pos",pt.cex = 1,axis.size = 1,pch=19)
# add lines
pos_fsts<-pf1[pf1$Fst>0,]
arrows(x0=pos_fsts$plot.pos,x1=pos_fsts$plot.pos,
       y0=pos_fsts$low_ci, y1=pos_fsts$upp_ci,
       col=c("darkgrey","lightgrey")[as.factor(pos_fsts$Chrom)],length=0)
```

Ok, so I calculated the confidence intervals around the permuted means, not the actual Fst values. 

So, how do I present these results? possibly not CIs. A difference between actual and permuted?

```{r}
par(mfrow=c(2,1))
pf1a<-fst.plot(permuted_fsts[[1]],fst.name = "Fst",bp.name="Pos",pt.cex = 1,axis.size = 1,pch=19,y.lim = c(0,1))
pf1p<-fst.plot(permuted_fsts[[1]],fst.name = "mean_perm",bp.name="Pos",pt.cex = 1,axis.size = 1,pch=19,y.lim = c(0,1))
arrows(x0=pf1p$plot.pos,x1=pf1p$plot.pos,
       y0=pf1p$low_ci, y1=pf1p$upp_ci,
       col=c("darkgrey","lightgrey")[as.numeric(pf1p$Chrom)%%2],length=0)
```

So they definitely are different. Maybe I can do a chi-squared test or something.


```{r}
fwsw.al<-read.delim("stacks/populations_subset75/batch_2.fst_ALFW-ALST.tsv")
fwsw.la<-read.delim("stacks/populations_subset75/batch_2.fst_ALST-LAFW.tsv")
fwsw.tx<-read.delim("stacks/populations_subset75/batch_2.fst_TXCC-TXFW.tsv")
fwsw.fl<-read.delim("stacks/populations_subset75/batch_2.fst_FLCC-FLFW.tsv")
```



# 9 September 2019

Ok, so that permuted Fsts thing works for permutations, but I think something odd is going on with the Fst calculations.

```{r}
permute.gwsca<-function(vcf,map1,nperms,z=1.96, maf.cutoff = 0.05,cov.thresh=0.2){
  # calculate the actuals
  actual_fsts<-gwsca(vcf,colnames(vcf)[1:9],
                     map1[map1[,2] %in% unique(map1[,2])[1],1],
                     map1[map1[,2] %in% unique(map1[,2])[2],1],
                     maf.cutoff=maf.cutoff,prop.ind.thresh=cov.thresh)
  # do the permutations
  perm_fsts<-lapply(1:nperms,function(i,vcf,map1){
    perm_map<-map1
    perm_map[,2]<-perm_map[,2][permute::shuffle(perm_map[,2])]
    perm_dat<-gwsca(vcf,colnames(vcf)[1:9],
                     perm_map[perm_map[,2] %in% unique(perm_map[,2])[1],1],
                     perm_map[perm_map[,2] %in% unique(perm_map[,2])[2],1],
                     maf.cutoff,cov.thresh)
   
    return(perm_dat)
  },vcf=vcf,map1=map1)
  
  # calculate stats
  fsts<-t(do.call(rbind,lapply(perm_fsts,'[[',"Fst"))) #extract permuted fsts
  perm_fst_mu<-rowMeans(fsts)
  perm_fst_sd<-apply(fsts,1,sd)
  perm_fst_ci<-z*perm_fst_sd/sqrt(nperms)
  fst_dat<-data.frame(cbind(actual_fsts,
                            n_perms=nperms,
                            mean_perm=perm_fst_mu,
                            low_ci=perm_fst_mu-perm_fst_ci,
                            upp_ci=perm_fst_mu+perm_fst_ci))
  return(fst_dat)
}
```
```{r}
source("../../gwscaR/R/gwscaR.R")
source("../../gwscaR/R/gwscaR_plot.R")
source("../../gwscaR/R/gwscaR_utility.R")
source("../../gwscaR/R/gwscaR_fsts.R")
source("../../gwscaR/R/gwscaR_popgen.R")
source("../../gwscaR/R/vcf2dadi.R")
```
```{r}

permuted_fsts<-readRDS("permuted_fsts.RDS")
vcf<-parse.vcf("stacks/populations_subset75/batch_2.pruned.vcf")
popmap<-data.frame(inds=colnames(vcf)[10:ncol(vcf)],
                   pops=gsub("sample_(\\w{4}).*","\\1",colnames(vcf)[10:ncol(vcf)]),
                   stringsAsFactors = FALSE)
pwise_maps<-list(popmap[popmap$pops %in% c("TXFW","TXCC"),],
                 popmap[popmap$pops %in% c("FLLG","FLCC"),],
                 popmap[popmap$pops %in% c("ALFW","ALST"),],
                 popmap[popmap$pops %in% c("LAFW","ALST"),])

TXmap<-popmap[popmap$pops %in% c("TXFW","TXCC"),]
perm_test<-permute.gwsca(vcf,TXmap,10,maf.cutoff=0,cov.thresh = 0)
```

Ok, now let's dive into this analysis. Using browser(), I deduced that this was a minimum allele frequency issue. Because the loci in this analysis have passed a global minor allele frequency threshold, I'm going to run with a maf=0. The other reason is that some of them are polymorphic only in one population, and the function currently only deals with loci that are polymoprhic in both populations. This is unfortunately most of them in the TX dataset so I might change the function. 

Now it seems to be working correctly so I'll apply it to the larger dataset.

```{r}
permuted_fsts<-lapply(pwise_maps,permute.gwsca,vcf=vcf,nperms=100, maf.cutoff=0)
saveRDS(permuted_fsts,"permuted_fsts.RDS")
```


# 5 September 2019

Returning to the idea of permutations...

I'll want to permute the labels in each pairwise comparison.

```{r}
vcf<-parse.vcf("stacks/populations_subset75/batch_2.pruned.vcf")

```

In some ways this might be easier to run in populations, but I want to try it here.

My gwscaR code isn't quite right for this because the population lists must be found in the individual IDs. So I need to re-make this using a population map framework. So here's the population map:

```{r}
popmap<-data.frame(inds=colnames(vcf)[10:ncol(vcf)],
                   pops=gsub("sample_(\\w{4}).*","\\1",colnames(vcf)[10:ncol(vcf)]),
                   stringsAsFactors = FALSE)
```

Now I want to do pairwise comparisons and permute those. 

```{r}
pwise_maps<-list(popmap[popmap$pops %in% c("TXFW","TXCC"),],
                 popmap[popmap$pops %in% c("FLLG","FLCC"),],
                 popmap[popmap$pops %in% c("ALFW","ALST"),],
                 popmap[popmap$pops %in% c("LAFW","ALST"),])
```

`gwscaR::fst.one.vcf` actually will work well -- as will `gwscaR::gwsca`. What I need to do for permutations is for each of these pairwise maps, I need to run a permutation. 

```{r}
permute.gwsca<-function(vcf,map1,nperms,z=1.96, maf.cutoff = 0.05,cov.thresh=0.2){
  # calculate the actuals
  actual_fsts<-gwsca(vcf,colnames(vcf)[1:9],
                     map1[map1[,2] %in% unique(map1[,2])[1],1],
                     map1[map1[,2] %in% unique(map1[,2])[2],1],
                     maf.cutoff,cov.thresh)
  # do the permutations
  perm_fsts<-lapply(1:nperms,function(i,vcf,map1){
    perm_map<-map1
    perm_map[,2]<-perm_map[,2][permute::shuffle(perm_map[,2])]
    return(gwsca(vcf,colnames(vcf)[1:9],
                     perm_map[perm_map[,2] %in% unique(perm_map[,2])[1],1],
                     perm_map[perm_map[,2] %in% unique(perm_map[,2])[2],1],
                     maf.cutoff,cov.thresh))
  },vcf=vcf,map1=map1)
  # calculate stats
  fsts<-t(do.call(rbind,lapply(perm_fsts,'[[',"Fst"))) #extract permuted fsts
  perm_fst_mu<-rowMeans(fsts)
  perm_fst_sd<-apply(fsts,1,sd)
  perm_fst_ci<-z*perm_fst_sd/sqrt(nperms)
  fst_dat<-data.frame(cbind(actual_fsts,
                            n_perms=nperms,
                            mean_perm=perm_fst_mu,
                            low_ci=perm_fst_mu-perm_fst_ci,
                            upp_ci=perm_fst_mu+perm_fst_ci))
  return(fst_dat)
}
```

```{r}
TXmap<-popmap[popmap$pops %in% c("TXFW","TXCC"),]
perm_test<-permute.gwsca(vcf,TXmap,10)

```

This seems like it will work, I think!

Let's try it on a larger scale:

```{r}
permuted_fsts<-lapply(pwise_maps,permute.gwsca,vcf=vcf,nperms=100)
saveRDS(permuted_fsts,"permuted_fsts.RDS")
```
Ok, I want to do permutations but I can't think about how to implement it right now. Looking at this, though there seems to be an issue, since Hs1 and Hs2 are non-zero but Hs, Ht, and Fst are 0 for some of these. I'll need to dig into this tomorrow.

# 28 August 2019

loading the file with diveRsity is pretty slow so I'm going to try subsetting it first. I'm gonna give the R package `genepopedit` a try.

```{r, eval=FALSE}
devtools::install_github("rystanley/genepopedit") #https://github.com/rystanley/genepopedit
```

```{r}
library(genepopedit)
source("../../gwscaR/R/gwscaR.R")
source("../../gwscaR/R/gwscaR_plot.R")
source("../../gwscaR/R/gwscaR_utility.R")
source("../../gwscaR/R/gwscaR_fsts.R")
source("../../gwscaR/R/gwscaR_popgen.R")
source("../../gwscaR/R/vcf2dadi.R")
library(fsthet)
source("../R/203_treemix_plotting_funcs.R")#I've modified these functions
library(knitr)
library(scales)

```
```{r}
gpop75<-my.read.genepop("stacks/populations_subset75/batch_2.genepop")
vcf<-parse.vcf("stacks/populations_subset75/batch_2.pruned.vcf")
```

It's not clear to me how to match up the genepop locus names and the IDs in the vcf file. I'm guessing that the genepop ones are ID_BP but I'm not sure. 

```{r}
gpop_ids<-as.numeric(gsub("X(\\d+)_\\d+","\\1",colnames(gpop75)[3:ncol(gpop75)]))
```
 Yes I think that's the case. `r length(gpop_ids %in% vcf$ID)` of the `r length(gpop_ids)` IDs are in the vcf file. and `r length(vcf$ID %in% gpop_ids)` of the `r length(vcf$ID)` vcf IDs are in the genepop file. The issue is that I don't know which ones I chose! Because it was done randomly and the POS is not the same as the basepair tacked onto the genepop IDs -- POS indicates the position on the chromosome, whereas the number in the genepop names is the location on the 95-bp locus.
 
The map file might be able to help me convert them

```{r}
map<-read.delim("stacks/populations_subset75/batch_2.plink.map",comment.char = "#",header=FALSE)
colnames(map)<-c("CHROM","ID_BP","X","POS")
map$ID<-as.numeric(gsub("(\\d+)_(\\d+)","\\1",map$ID_BP))
map$BP<-as.numeric(gsub("(\\d+)_(\\d+)","\\2",map$ID_BP))
# Map positions start from 1, vcf from 0
map_pruned<-map[paste(map$POS+1,map$ID,sep="_") %in% paste(vcf$POS, vcf$ID,sep="_"),]
gpop75_pruned<-gpop75[,c(colnames(gpop75)[1:2],colnames(gpop75)[colnames(gpop75) %in% paste0("X",map_pruned$ID_BP)])]
```

Woohoo, this seems to have worked!

Now I need to turn this back into genepop file format to work with diveRsity...

```{r}
library(genepopedit)
genepop_unflatten(gpop75_pruned,"stacks/populations_subset75/batch_2.pruned.genepop")

```

Ok, this seems to work. 

```{r}
library(diveRsity)
source("../R/readGenepop_modified.R") # need to load this after divRsity to mask the package fx
pop75_div<-fastDivPart(infile=gpop75_pruned,gp = 2) #readGenepop(infile="stacks/populations_subset75/batch_2.pruned.genepop",gp = 2)
```

This is not working! I ended up re-writing some of the code that parses the genepop files because diveRsity wasn't correctly parsing my genepop file for some reason (something to do with the POP specifications, as far as I can tell -- which came from genepopedit and before that from Stacks).

However, this does not get past the issues, as far as I can tell. So perhaps I should retire the idea of using this package and/or post an issue on the github. I think I should maybe abandon this and perform permutations/bootstraps on my own. 

So if I'm going to do permutations or bootstraps I might have to write it myself.

```{r}
popmap<-data.frame(pop=gsub("sample_([A-Z]{4}).*","\\1",colnames(vcf)[10:ncol(vcf)]),
                   sample=colnames(vcf)[10:ncol(vcf)],stringsAsFactors = FALSE)

```



### Pcadapt

I managed to run pcadapt again but I need a better overall plan before doing anything with the outliers.



# 27 August 2019

I'm going to try the diveRsity package.

```{r}
library(diveRsity)
pop75<-readGenepop("stacks/populations_subset75/batch_2.genepop")
vcf<-parse.vcf("stacks/populations_subset75/batch_2.pruned.vcf")

divstats<-bigDivPart(gpop75)
```

This is taking far too long. I should try this at work tomorrow.


# 19 August 2019

I'm running the heterogeneous migration models for FL on my local machine and today requested that dadi be installed on abacus. I've also prepared scripts to run FLFW vs ALFW, FLFW vs TXFW, and TXFW vs ALFW. I may need to modify those scripts further if I'm going to run them on abacus, but for now they're a start.


# 14 August 2019

I've been putting off returning to this because I don't really know what to do. With regards to dadi, I think there are a few things I need to do:

1. Incorporate admixture into models. 
2. Run models between freshwater populations, specifically TX vs FL, TX vs AL/LA, and FL vs AL/LA. I'm worried about a 3D model taking forever, so maybe run AL vs LA? Ugh this will take FOREVER. Maybe I can figure out how to run these on the server?

Ok, I'm going to read that Rougeux paper and consider using his models. Rougeux et al used 4 basic models with 13 extensions. They used strict isolation (SI), isolation with migration (IM) ancient migration (AM) and secondary contact (SC). The extensions capture effect of selection inducing reduced gene flow around loci associated with adaptive divergence. 

The way this is written makes me think that they fit the basic models and then incorporated heterogeneous gene flow plus background selection (considering local variation in Ne). So, I'm going to try to use their code to add variation in migration and Ne across the genome into the asym_mig code. I think I've done this successfully! So now to run it for FL...I've updated the 252_2DFL.py script and now will try running it on my computer. 

# 9 August 2019

Gutenkunst wrote back, saying "The suitability of dadi for this depends on what assumptions you’re willing to make. Are you assuming your RADseq loci are independent? If not, you’ll need a much more complex simulation scheme to capture that dependence anyways. If yes, then you don’t need to use coalescent simulations." I guess he doesn't want to just answer my questions. 

But I guess it brings me back to the question of what I'm even doing, in a way. Maybe I should focus on running the models with heterogeneous selection first. Do I even need to identify outliers? probably not to this extent. Blargh. I should probably run some additional models. 

I am looking at Rougeux's code: https://github.com/crougeux/Dadi_v1.6.3_modif/blob/master/Dadi_studied_model/00_inference/modeledemo_mis_new_models.py

for inspiration about the heterogeneous effective population sizes.

```{python}
def SI2N(params, (n1,n2), pts):
    nu1, nu2, Ts, nr, bf, O = params
    """
    Model with split and complete isolation, heterogenous effective population size
        (2 classes, shared by the two populations = background selection)
    nu1: Size of population 1 after split.
    nu2: Size of population 2 after split.
    Ts: The scaled time of the split
    n1,n2: Size of fs to generate.
    nr: Proportion of non/low-recombining regions
    bf : Background factor (to which extent the effective population size is reduced in the non-recombining regions)
    O: The proportion of accurate orientation
    pts: Number of points to use in grid for evaluation.
    """
    # Define the grid we'll use
    xx = dadi.Numerics.default_grid(pts)

    # Spectrum of non-recombining regions
    # phi for the equilibrium ancestral population
    phinr = dadi.PhiManip.phi_1D(xx)
    # Now do the divergence event
    phinr = dadi.PhiManip.phi_1D_to_2D(xx, phinr)
    # We set the population sizes after the split to nu1 and nu2	
    phinr = dadi.Integration.two_pops(phinr, xx, Ts, nu1*bf, nu2*bf, m12=0, m21=0)
    # Finally, calculate the spectrum.
    # oriented
    fsnrO = dadi.Spectrum.from_phi(phinr, (n1,n2), (xx,xx))
    # mis-oriented
    fsnrM = dadi.Numerics.reverse_array(fsnrO)
    
    
	# Spectrum of recombining regions
    # phi for the equilibrium ancestral population
    phir = dadi.PhiManip.phi_1D(xx)
    # Now do the divergence event
    phir = dadi.PhiManip.phi_1D_to_2D(xx, phir)
    # We set the population sizes after the split to nu1 and nu2
    phir = dadi.Integration.two_pops(phir, xx, Ts, nu1, nu2, m12=0, m21=0)
    # Finally, calculate the spectrum.
    # oriented
    fsrO = dadi.Spectrum.from_phi(phir, (n1,n2), (xx,xx))
    # mis-oriented
    fsrM = dadi.Numerics.reverse_array(fsrO)

    ### Sum the two spectra in proportion O
    fs= O*(nr*fsnrO + (1-nr)*fsrO) + (1-O) *(nr*fsnrM + (1-nr)*fsrM)

    return fs

```

I should be able to adapt this in my code, but honestly 4:30pm on a Friday is not the time to be doing this. I will work on this on Monday, I think.

Remember to be inspired by this paper: https://onlinelibrary.wiley.com/doi/full/10.1111/jeb.13482

# 8 August 2019

I'm using the code in 257_debug_ms.py to make the comparison plots.

Other things I can do: 
1. re-evaluate calculation of theta by scouring the dadi user webpage
2. run many mas sims and average the results (or increase iter?)

regarding theta, on this group (https://groups.google.com/forum/#!searchin/dadi-user/dadi$20to$20ms|sort:date/dadi-user/DYrpTHCcC_I/nl3f2eGSAQAJ) Ryan says to use  `L = 3e6 * 56,343/123,797` for someone with 3 million sites (RAD loci), a total of 123797 SNPs, of which 56343 were used. That L should then get plugged into `4*Ne*mu*L`, and dadi's theta is `4*Ne*mu*L`. So to get the ms theta, which is `4*Ne*mu*L`, we divide dadi's theta by L. So what I need to do is get L. Let's write out the variables. `L = (num loci)*(num SNPs in analysis)*(total num SNPs)` - which is different from what I did (`dadi-theta*Llocus/Ltotal` [`227.83*95/(12103*251339*95/1370051)=0.1026112`]), and does not include the length of the loci.

However, this post (https://groups.google.com/forum/#!searchin/dadi-user/dadi$20to$20ms|sort:date/dadi-user/1Chc8kXrWE0/NuB4d4X2BwAJ) uses the length of the SNPs. They did `dadi-theta*(length of sequence)/(total length of sequences used) `

I've increased the iter substantially and it is very slow, but when it is done I will test out averaging spectra (just try doing mean() with two spectra objects I guess?). Using mean does not work! Nor does dadi.Spectrum.mean(fs1,fs2)

So I posted this on the forum:

```

Thank you for your response! I have a few follow-up questions:

  1.  To calculate theta I was following what the original post had done, which was: (dadi's theta estimate)*(individual rad locus length)/(total estimated length (L)), where L = (length of RAD loci)*(total number of RAD loci)*(number of SNPs in the analysis)/(total number of SNPs). Your response prompted me to scour the dadi-user group more thoroughly and I see that this estimate differs slightly from others that have been suggested. For instance, in this post, the conversion to ms theta is (daid-theta)*(length of sequence)/(total length of sequence used), and in this post, the conversion appears to be (dadi-theta)/L, where L = (total number of rad loci)*(number of SNPs in the analysis)*(total number of SNPs). What would you say is the best way to convert dadi's estimated theta to the input theta for ms for RAD-seq data?
  2.  I think I am a bit confused about specifying the number of loci in ms. I thought the number of iterations is equivalent to the number of SNPs simulated by ms (and I had used the number of SNPs in my dataset) -- is there a different way to specify the number of SNPs, and the number of SNPs per locus, in ms?
   3. How do you recommend that I average the spectra produced by multiple ms runs to compare the models?

Thank you so much for your help and the time you put into answering all of the questions here on the forum!

```

And now I just need to wait for Gutenkunst to answer.

He responded already! He said:

```
Maybe it’s time to back up here. Why are you running ms again? Typically we would only do coalescent simulations if we wanted to test how linkage affects our data. But if you’re simulating RAD seq loci independently and only taking one SNP per locus, then you’re generating data with no linkage anyways. In this case, the only difference between dadi and ms would be if you made an error in converting model specifications.

If you’re assuming you’re loci are unlinked, then the likelihood dadi calculates is correct, and you can do model selection with likelihood ratio tests, etc.
```

So I responded:

```
I would like to compare the distribution of Fsts in the observed data to a null distribution, so I'm trying to simulate data using the best-inferred model for my dataset. I was under the impression that dadi does not simulate data that can be used for this purpose, so I'm trying to use ms to generate simulated data. Do you have a suggestion for a better way to do this?
```

Now it's the waiting game again

# 1 August 2019

This is the response from Ryan Gutenkunst:
```
Hi Sarah,

Your demographic parameter conversion looks fine to me. But I don’t follow the theta conversion formula you’re using. You want your bootstrap data sets to have the same total theta as your original data data set. Are you downsampling the ms simulations to get to one SNP per locus?

It’s hard to tell whether your ms and dadi simulations agree, because the ms simulation is so sparse. Run many ms simulations then average the results to compare with the dadi model (and look at the residuals to see if there is systematic difference).

Best,
Ryan
```

And I find that response to be rather confusing. So....??? A few things to unpack:
1. Check that I've calculated theta correctly and re-write the equation.
2. How many SNPs per locus am I specifying in ms??
3. Run more ms sims and average the results 
4. How do I look at residuals? 
    Using dadi.Plotting.plot_2d_comp_multinomial between the data (fl_optimized) and models looks like it should work. I was struggling at first because I'd used different projection #s but I think it's better now?
    
I'm finding this VERY frustrating so I'm going to move onto something else for right now.


# 31 July 2019

I got ms to work within dadi by adding the msdir to the $PATH (duh). The issue was that python couldn't find the ms program.
Ok, so now troubleshooting the odd looking spectrum. I need to compare model with different cases, both the ms model and the dadi model. 
After trying a number of different models and still not finding a particularly good fit, I found that the most effective approach is to increase theta -- which implies that my estimates are not very good.

If I remove the -ej part of the ms code (but keep migration and ns) it works better. So I should probably check that -ej is the appropriate way to specify these models in the ms manual. I feel fairly confident that this is specified correctly...perhaps I should post my issue on the google group and get some feedback. 

I posted this on the dadi forum, in response to the thread from 2018 that inspired me:

```
Were you able to resolve this issue? I am running into a similar issue. In my case, I have ddRADseq data and I generated 251339 RAD loci (each 95bp long) with a total of 1370051 SNPs. I filtered and down-sampled to a dataset with 12103 unpolarized SNPs, each from a different RAD locus. I compared a number of models using Daniel Portik's dadi-pipeline and a model with ongoing asymmetric migration since a population split had the best fit.

The following forward-in-time dadi parameters were estimated from the best-fitting model:

Size of population 1 after split (nu1) = 0.1317
Size of population 2 after split (nu2) = 8.4225
Time in the past of split in units of 2*Na generations (T) = 0.1441
Migration from pop 2 to pop 1 (m12) = 0.7777
Migration from pop 1 to pop 2 (m21) = 0.0561
theta = 227.83

Based on what I've seen on this forum, I converted these parameters to ms arguments as follows:
-m 1 2 1.5554 [m12*2]
-m 2 1 0.1122 [m21*2]
-n 1 0.1317
-n 2 0.84225
-ej 0.07205 2 1 [T/2]

For theta, I converted it using the previously-recommended formula of (dadiTheta * locusLength) / ( (numSNPused * numLoci * locusLength) / totalNumSNPs ), which was:
-t = (227.83  * 95) / ( (12103 * 251339 * 95) / 1370051 )  = 0.1026112

like this:
core="-m 1 2 1.5554 -m 2 1 0.1122 -n 1 0.1317 -n 2 8.4225 -ej 0.07205 2 1"
command=dadi.Misc.ms_command(theta=0.1026112, ns=(60,70), core=core, iter=12103)
ms_fs=dadi.Spectrum.from_ms_file(os.popen(command))
folded_sfs=dadi.Spectrum.fold(ms_fs)
dadi.Plotting.plot_single_2d_sfs(folded_sfs,vmin=0.000001)


The sfs of asym_mig with these parameters looks like this:

FL_asym_mig_folded.png

But the output of the ms run above looks like this:

FL_ms_asym_mig_folded.png


Following the suggestions in this post, I tested it with equal migration between the two populations, no migration, and without the -ej term (the attached script has all of the code for these exploratory analysis). I found that removing the -ej term improved the ms-simulated sfs:

FL_ms_asym_mig_folded_noEj.png
However I'm not convinced that this ms model is the appropriate one for my dadi model.

Is anyone able to spot an issue in my implementation of the ms code? Has anyone successfully solved this type of issue?

Thank you in advance for your help!

Sarah

Attached: 257_debug_ms.py
```


Here are some of the things I tried:

```{python}
def no_divergence(notused, ns, pts):
    """
    Standard neutral model, populations never diverge.
    """
    
    xx = Numerics.default_grid(pts)
    
    phi = PhiManip.phi_1D(xx)
    phi = PhiManip.phi_1D_to_2D(xx, phi)
    
    fs = Spectrum.from_phi(phi, ns, (xx,xx))
    return fs


def no_mig(params, ns, pts):
    """
    Split into two populations, no migration.

    nu1: Size of population 1 after split.
    nu2: Size of population 2 after split.
    T: Time in the past of split (in units of 2*Na generations) 
    """
    nu1, nu2, T = params

    xx = Numerics.default_grid(pts)

    phi = PhiManip.phi_1D(xx)
    phi = PhiManip.phi_1D_to_2D(xx, phi)

    phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=0, m21=0)

    fs = Spectrum.from_phi(phi, ns, (xx,xx))
    return fs


def sym_mig(params, ns, pts):
    """
    Split into two populations, with symmetric migration.

    nu1: Size of population 1 after split.
    nu2: Size of population 2 after split.
    T: Time in the past of split (in units of 2*Na generations) 
    m: Migration rate between populations (2*Na*m)
    """
    nu1, nu2, m, T = params

    xx = Numerics.default_grid(pts)

    phi = PhiManip.phi_1D(xx)
    phi = PhiManip.phi_1D_to_2D(xx, phi)

    phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m, m21=m)

    fs = Spectrum.from_phi(phi, ns, (xx,xx))
    return fs

# Let's try it without the asymmetric migration
m = mean([m12,m21]) #0.4169
sym_mig_fs = sym_mig(params=(nu1,nu2,m,T),ns=ns,pts=pts)
sym_mig_folded = dadi.Spectrum.fold(sym_mig_fs)
dadi.Plotting.plot_single_2d_sfs(sym_mig_folded,vmin=0.000001)

core="-m 1 2 0.8338 -m 2 1 0.8338 -n 1 0.1317 -n 2 8.4225 -ej 0.07205 2 1"
command=dadi.Misc.ms_command(theta=0.1026112, ns=(60,70), core=core, iter=12103)
ms_fs=dadi.Spectrum.from_ms_file(os.popen(command))
folded_sfs=dadi.Spectrum.fold(ms_fs)
dadi.Plotting.plot_single_2d_sfs(folded_sfs,vmin=0.000001) # this is still not great.


# no migration

no_mig_fs = no_mig(params=(nu1,nu2,T),ns=ns,pts=pts)
no_mig_folded = dadi.Spectrum.fold(no_mig_fs)
dadi.Plotting.plot_single_2d_sfs(no_mig_folded,vmin=0.000001)

core="-n 1 0.1317 -n 2 8.4225 -ej 0.07205 2 1"
command=dadi.Misc.ms_command(theta=0.1026112, ns=(60,70), core=core, iter=12103)
ms_fs=dadi.Spectrum.from_ms_file(os.popen(command))
folded_sfs=dadi.Spectrum.fold(ms_fs)
dadi.Plotting.plot_single_2d_sfs(folded_sfs,vmin=0.000001) # this is still not great.

# no divergence -- ms can't converge
no_div_fs = no_divergence([nu1,nu2],ns=ns,pts=pts)
no_div_folded = dadi.Spectrum.fold(no_div_fs)
dadi.Plotting.plot_single_2d_sfs(no_div_folded,vmin=0.000001)

core="-n 1 0.1317 -n 2 8.4225"
command=dadi.Misc.ms_command(theta=0.1026112, ns=(60,70), core=core, iter=12103)
ms_fs=dadi.Spectrum.from_ms_file(os.popen(command))
folded_sfs=dadi.Spectrum.fold(ms_fs)
dadi.Plotting.plot_single_2d_sfs(folded_sfs,vmin=0.000001) # this is still not great.

# what about a different theta?
core="-m 1 2 0.8338 -m 2 1 0.8338 -n 1 0.1317 -n 2 8.4225 -ej 0.07205 2 1"
command=dadi.Misc.ms_command(theta=.05, ns=(60,70), core=core, iter=12103)
ms_fs=dadi.Spectrum.from_ms_file(os.popen(command))
folded_sfs=dadi.Spectrum.fold(ms_fs)
dadi.Plotting.plot_single_2d_sfs(folded_sfs,vmin=0.000001) # this is still not great.

# a different time + no migration? ms t = dadi T/2
asym_mig_fs = asym_mig(params=(nu1,nu2,0,0,1),ns=ns,pts=pts)
asym_mig_folded = dadi.Spectrum.fold(asym_mig_fs)
dadi.Plotting.plot_single_2d_sfs(asym_mig_folded,vmin=0.000001)

core="-m 1 2 0 -m 2 1 0 -n 1 0.1317 -n 2 8.4225 -ej 0.5 2 1"
command=dadi.Misc.ms_command(theta=0.1026112, ns=(60,70), core=core, iter=12103)
ms_fs=dadi.Spectrum.from_ms_file(os.popen(command))
folded_sfs=dadi.Spectrum.fold(ms_fs)
dadi.Plotting.plot_single_2d_sfs(folded_sfs,vmin=0.000001)

# a different time + sym migration? ms t = dadi T/2
asym_mig_fs = asym_mig(params=(nu1,nu2,m12,m12,1),ns=ns,pts=pts)
asym_mig_folded = dadi.Spectrum.fold(asym_mig_fs)
dadi.Plotting.plot_single_2d_sfs(asym_mig_folded,vmin=0.000001)

core="-m 1 2 1.5554 -m 2 1 1.5554 -n 1 0.1317 -n 2 8.4225 -ej 0.5 2 1"
command=dadi.Misc.ms_command(theta=0.1026112, ns=(60,70), core=core, iter=12103)
ms_fs=dadi.Spectrum.from_ms_file(os.popen(command))
folded_sfs=dadi.Spectrum.fold(ms_fs)
dadi.Plotting.plot_single_2d_sfs(folded_sfs,vmin=0.000001)
# Increasing theta seems to help -- perhaps my theta estimate is not appropriate?


os.chdir("~/Research/popgen/fwsw_results/dadi_analysis")
dd = dadi.Misc.make_data_dict ( "fwsw75.dadi.snps" )
fl = dadi.Spectrum.from_data_dict(dd , pop_ids =[ 'FLLG','FLCC' ],projections =[70,61] ,polarized = False )  #polarized = False creates folded spectrum

os.chdir(fl_dir)

pts = [ 220,240 ]

#Provide best optimized parameter set for empirical data.
#These will come from previous analyses you have already completed (above)
emp_params = [0.1317,8.4225,0.7777,0.0561,0.1441]

#Indicate whether your frequency spectrum object is folded (True) or unfolded (False)
fs_folded = True

#Fit the model using these parameters and return the folded model SFS (scaled by theta).
#Here, you will want to change the "sym_mig" and sym_mig arguments to match your model function,
#but everything else can stay as it is. See above for argument explanations.
scaled_fl = Optimize_Empirical(fl, pts, "Empirical", "asym_mig", asym_mig, emp_params, fs_folded=fs_folded)




```

Also worth keeping, for inspiration/guidance, is this github site that converts ms into fst type data:
https://github.com/molpopgen/msstats

# 30 July 2019

I'm moving forward thinking about how to use the dadi simulations to analyze my Fst results (with the FL population as my first pass).
The dadi simulations only give me a sense of whether my parameters are 'good' or not, and I think they are (given the chi-squared distributions etc). I should maybe also look at the parameters as well. Once I'm confident in my parameter estimates, I'll use them to run ms. 

So looking at histograms of the simulated values, the parameters I used definitely fall within the distributions. The m21 parameter seems a bit low comopared to many of the estimates, since the median estimate is 0.3838, the variance is 0.03662088, and my initial parameter was 0.0561. I'll run with it for now, but I should maybe test different runs of ms just to be sure.

To run ms I need to estimate theta (which comes from the time parameter??) and 4N0m. Also, the population sizes need to be relative to N0. In the dadi manual, it states:
```
So to convert from a time in ∂a∂i to a time in ms, divide by 2.
Migration rates are given in units of M ij = 2N ref m ij . Again, this differs from ms, where
the scaling factor is 4N ref generations. So to get equivalent migration (m ij ) in ms for a given
rate in ∂a∂i, multiply by 2
```

Therefore, for Florida, I need to use time = `0.1441/2`, m12 = `0.7777*2`, m21 = `0.0561*2`.  

On the dadi forum was a question posed using this model (https://groups.google.com/forum/#!searchin/dadi-user/ms%7Csort:date/dadi-user/N623Qe75iDs/neuuqEyUAwAJ):

```

Forward in time dadi parameters:
theta = 5911.67
pop 1 relative pop size post divergence = .1009
pop 2 relative pop size post divergence = .5933
T1, time from divergence until gene flow ceases = 5.3655
T2, time from cessation of gene flow until present = 0.0269
m12 = migration from pop 2 into 1 = 1.1616
m21 = migration from pop 1 into 2 = 0.4132
pop 1 relative pop size after size change = 29.9198
pop 2 relative pop size after size change = 8.7145

To simulate unlinked RAD loci, I generate a single-locus ms command by adjusting theta to be locus specific: 5911.67 * 94/3451628, i.e. multiply theta x individual rad locus length/total estimated length (L).


MS SIM SETUP
Basically, I simulate the set of unlinked rad loci and sum their respective sfs:

ms_fs = dadi.Spectrum.from_ms_file(popen('ms 306 56055 -t 0.160995617141 -I 2 91 215 -n 1 29.9198 -n 2 8.7145 -en 0.01345 1 0.1009 -en 0.01345 2 0.5933 -em 0.01345 1 2 0.8264 -em 0.01345 2 1 2.3232 -ej 2.6962 1 2),average=False)
folded_sfs=dadi.Spectrum.fold(ms_fs)
fig = pylab.figure(1)
fig.clear()
dadi.Plotting.plot_single_2d_sfs(folded_sfs,vmin=0.005)

note that the vmin setting was the same used for the plotting of the data and dadi inferred sfs.

MS ARGS IN DETAIL
-n 1 29.9198 : relative pop size of pop 1 (post expansion) at present
-n 2 8.7145 : relative pop size of pop 2 (post expansion) at present
-en 0.01345 1 0.1009: going backward in time, pop 1 size changes, in ms units, at 0.0269/2 = 0.01345 to .1009
-en 0.01345 2 0.5933 : similarly, pop 2 size changes at same time as pop 1 to .5933 

### time parameter conversions ###
dadi parameters are forward in time, while ms is backwards. m12-dadi, the fraction of pop 1 comprised of migrants from pop 2, is equivalent in coalescent terms to m21 for ms. thus, given that dadi rates are multiplied by 2 to get equivalent ms rates:

 -em 0.1345 1 2 0.8264  : M12, the # of individuals from pop 1 comprised of backward in time migrants from pop 2
 -em 0.1345 2 1 2.3232 : M21 the # of individuals from pop 2 comprised of backward in time migrants from pop 1
 -ej 2.6962 1 2 : the full length of the genealogy in ms units is (T1+T2)/2 = (5.3655 + 0.0269)/2 = 2.6962, at which time all lineages from population 1 are moved to population 2
 
```

So what this tells me is that I can run ms from within dadi!
And The conversions I need to make are not exactly what I had thought. Information I need: number of total RAD loci and length of RAD loci. 

Then, I'll have the following parameters:

nsam = n1 + n2 [82 + 94]
nreps = NUMSNPS [12103]
-t = `dadi-theta*Llocus/Ltotal` [`227.83*95/(12103*251339*95/1370051)=0.1026112`]
-m 1 2 = `2*m12` [`2*0.7777`=1.5554]
-m 2 1 = `2*m21` [`2*0.0561`=0.1122]
-n 1 nu1 [0.1317] (size of pop 1 after split at time T)
-n 2 nu2 [2 8.4225] (size of pop 2 after split at time T)
-ej T/2 2 1 [`0.1441/2`=0.07205 2 1] (combine the populations at time T)

According to the last version of the manuscript, I had 194294 RAD loci, which are 95 bp long, and I have 12103 SNPs in the analysis. 
But according to the sstacks log, I have 251339 loci in the catalog, and there are 1370051 SNPs in batch_2.catalog.snps.tsv. So, I now have:

ms 176 12103 -t 0.1026112 -I 2 82 94 -m 1 2 1.5554 -m 2 1 0.1122 -en 0.07205 1 0.1317 -en 0.07205 2 8.4225

Ok, this is actually wrong -- the ns are the current population sizes, which formed at time T, so I need to use -ej. 
This should be the code:
ms 176 12103 -t 0.1026112 -I 2 82 94 -m 1 2 1.5554 -m 2 1 0.1122 -n 1 0.1317 -n 2 8.4225 -ej 0.07205 2 1

Now the question is whether I use command-line ms, the ms built into dadi, or msprime (which was recommended by Gutenkunst on another forum post). 

```{python}
import os
import dadi
ms_fs=dadi.Spectrum.from_ms_file(os.popen('ms 176 12103 -t 0.1026112 -I 2 82 94 -m 1 2 1.5554 -m 2 1 0.1122 -n 1 0.1317 -n 2 8.4225 -ej 0.07205 2 1'))
```
Running the above does not work. But the same commands run in ms, and I was able to read them into dadi - but the spectrum looks off. I probably need to troubleshoot like the OP of the above information did, but I'm going to come back to that.

I got this to work:


```{python}
import os
import dadi
command=dadi.Misc.ms_command(theta=0.1026112, ns=(82,94), core=core, iter=12103)
```

but I still get an error when trying to run ms in dadi. Tomorrow I will try to install msprime and give that a try.

I also found some models for heterogeneous migration across the genome:  https://github.com/crougeux/Dadi_v1.6.3_modif/blob/master/Dadi_studied_model/00_inference/modeledemo_mis_new_models.py

# 29 July 2019

Gutenkunst wrote back:

```
Hello Sarah,

Attached is the script I used look at your spectra more. You’ll see that the negative values in each of the integrated spectra and the final spectrum itself are very small. If we look at the plot, the only values < 1e-12 (and thus all the negative values) are in the lower right-hand corner. If not much of your data lies in that corner, then you have nothing to worry about.

Best,
Ryan
```

with this script attached:

```{python}
from asym_mig_test import *

print('Minimum value in fs200: {0}'.format(fs200.min()))
print('Minimum value in fs250: {0}'.format(fs250.min()))
print('Minimum value in fs300: {0}'.format(fs300.min()))

spectra = [fs200,fs250,fs300]
xs = [_.extrap_x for _ in spectra]
fs_extrap = dadi.Numerics.quadratic_extrap(spectra, xs)

print('Minimum value in extrapolated spectrum: {0}'.format(fs_extrap.min()))

dadi.Plotting.plot_single_2d_sfs(fs_extrap, vmin=1e-12)

```

![2D asym_mig spectrum]("asym_mig_test.png")

Just to double check that my data aren't in that lower right-hand corner, I re-plotted my TX data with vmin=1e-12 and got this:

![2D data spectrum]("dadi_analysis/TX2D/TX_2d_sfs_smallvmin.png")

And that plot also has empty space in the bottom right-hand corner, so I think it's ok. Which means that I can ignore the warnings and move forward with my analyses of the TX runs. How many have I run, and do I need to run more? I don't think so, I have a lot of runs in the TX2D folder.

Analyzing those, I find that sym_mig_size is the best model, with optimal parameters 5.6521,4.4595,1.0109,19.9392,0.4646,1.3392,0.4042 (nu1a,nu2a,nu1b,nu2b,m,T1,T2). So I've now started the simulation code (254_2DoptNsim.py) with the TX dataset. The next big question to answer (other than how can I speed up the 3D analysis?) is how to simulate data in a way that will help me interpret my Fst values. I'm not sure that I can easily do that with dadi, I might need to use ms.


# 22 July 2019

I submitted this script to the dadi webpage for Ryan Gutenkunst to help me analyze the spectra:

```{python 255_asym_mig}
'''
Running asym_mig to compare frequency spectra
'''

import os
import numpy
import dadi
import pylab
from datetime import datetime
from dadi import Numerics, PhiManip, Integration
from dadi.Spectrum_mod import Spectrum



def asym_mig(params, ns, pts):
    """
    Split into two populations, with different migration rates.

    nu1: Size of population 1 after split.
    nu2: Size of population 2 after split.
    T: Time in the past of split (in units of 2*Na generations) 
    m12: Migration from pop 2 to pop 1 (2*Na*m12)
    m21: Migration from pop 1 to pop 2
	"""
    nu1, nu2, m12, m21, T = params
    xx = Numerics.default_grid(pts)
    
    phi = PhiManip.phi_1D(xx)
    phi = PhiManip.phi_1D_to_2D(xx, phi)
    
    phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
    fs = Spectrum.from_phi(phi, ns, (xx,xx))
    
    return fs    


# Set the parameters (from a run that resulted in lots of warnings)
nu1 = 1.01
nu2 = 15.4299
m12 = 0.9824
m21 = 1.1719
T = 3.6611
ns = [46,60]

# First set of points
pts = 200
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs200 = Spectrum.from_phi(phi, ns, (xx,xx))

# Second set of points
pts = 250
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs250 = Spectrum.from_phi(phi, ns, (xx,xx))

# Third set of points
pts = 300
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs300 = Spectrum.from_phi(phi, ns, (xx,xx))


```

This is in response to his email:

```
Hello Sarah,

You'll need to compare the numerical values in the spectra, using numpy functions. If you're not comfortable doing that, I can help if you send a self-contained script that generates your model and spectra.

Best,
Ryan
```

# 19 July 2019

Ok, returning to how to actually run this stuff. I want to run something like:

```{python asym_mig_goal}
asym_mig(params=[1.01,15.4299,0.9824,1.1719,3.6611], ns=[46,60] , pts=[200]) #ns are sample sizes
```

but this throws an error, "TypeError: int() argument must be a string or a number, not 'list'"

I'm not sure where this is going wrong, but it seems to be in the creation of the pts. Maybe I should run asym_mig step by step? This is asym_mig:

```{python asym_migfxn}
def asym_mig(params, ns, pts):
    """
    Split into two populations, with different migration rates.

    nu1: Size of population 1 after split.
    nu2: Size of population 2 after split.
    T: Time in the past of split (in units of 2*Na generations) 
    m12: Migration from pop 2 to pop 1 (2*Na*m12)
    m21: Migration from pop 1 to pop 2
	"""
    nu1, nu2, m12, m21, T = params
    xx = Numerics.default_grid(pts)
    
    phi = PhiManip.phi_1D(xx)
    phi = PhiManip.phi_1D_to_2D(xx, phi)
    
    phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
    fs = Spectrum.from_phi(phi, ns, (xx,xx))
    
    return fs    
```

```{python python_setup}
import os
import numpy
import dadi
import pylab
from datetime import datetime

#use dportik's functions
#get the optimize functions
execfile("../../programs/dadi_pipeline-master/Two_Population_Pipeline/Optimize_Functions.py")
execfile( "../../programs/dadi_pipeline-master/Two_Population_Pipeline/Models_2D.py")
execfile("../../scripts/250_custom_dadi_models.py")


# Load the data
dd = dadi.Misc.make_data_dict ( "fwsw75.dadi.snps" )
#projections is sample size of alleles
#need to use MINIMUM projections

#pops = ['FLLG', 'FLCC', 'ALFW','ALST','LAFW','TXFW','TXCC']
#projs = [70,      61,     72,     70,    72,    46,     61]

tx = dadi.Spectrum.from_data_dict(dd , pop_ids =[ 'TXFW','TXCC' ],projections =[46,60] ,polarized = False )  #polarized = False creates folded spectrum

```

So I'll try, step by step in ipython (after loading python_setup):


```{python asym_migfxn}
nu1 = 1.01
nu2 = 15.4299
m12 = 0.9824
m21 = 1.1719
T = 3.6611
ns = [46,60]

# First set of points
pts = 200
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs200 = Spectrum.from_phi(phi, ns, (xx,xx))
    
```

Incredibly, this seemed to work, so now I'll run the other points settings (250,300) and plot the resulting spectra.

```{python pts}
# Second set of points
pts = 250
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs250 = Spectrum.from_phi(phi, ns, (xx,xx))

# Third set of points
pts = 300
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs300 = Spectrum.from_phi(phi, ns, (xx,xx))
```


```{python plot_pts_sfs}
dadi.Plotting.plot_single_2d_sfs(fs200,vmin=0.01)
dadi.Plotting.plot_single_2d_sfs(fs250,vmin=0.01)
dadi.Plotting.plot_single_2d_sfs(fs300,vmin=0.01)
```

![SFS with 200 points]("dadi_analysis/TX2D/asym_mig_200ptsTest.png")
![SFS with 250 points]("dadi_analysis/TX2D/asym_mig_250ptsTest.png")
![SFS with 300 points]("dadi_analysis/TX2D/asym_mig_300ptsTest.png")

I have no idea how to interpret these. They all look basically the same. I think maybe it's time to respond to Gutenkunst's suggestion.

First, actually, I'll try the same set of code but with parameters that didn't give warnings (`r opt_warns[sample(which(opt_warns$Model=="asym_mig" & opt_warns$Warning=="FALSE"),1),]`).

```{python asym_migGoodParams}
nu1 = 1.0101
nu2 = 8.5997
m12 = 0.3722
m21 = 0.8104
T = 0.7221
ns = [46,60]

# First set of points
pts = 200
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs200 = Spectrum.from_phi(phi, ns, (xx,xx))
    
# Second set of points
pts = 250
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs250 = Spectrum.from_phi(phi, ns, (xx,xx))

# Third set of points
pts = 300
xx = Numerics.default_grid(pts)
phi = PhiManip.phi_1D(xx)
phi = PhiManip.phi_1D_to_2D(xx, phi)
phi = Integration.two_pops(phi, xx, T, nu1, nu2, m12=m12, m21=m21)
fs300 = Spectrum.from_phi(phi, ns, (xx,xx))

# Plot
dadi.Plotting.plot_single_2d_sfs(fs200,vmin=0.01)
dadi.Plotting.plot_single_2d_sfs(fs250,vmin=0.01)
dadi.Plotting.plot_single_2d_sfs(fs300,vmin=0.01)
```

![Good Params SFS with 200 points]("dadi_analysis/TX2D/asym_mig_200ptsGoodParams.png")
![Good Params SFS with 250 points]("dadi_analysis/TX2D/asym_mig_250ptsGoodParams.png")
![Good Params SFS with 300 points]("dadi_analysis/TX2D/asym_mig_300ptsGoodParams.png")

I posted these results on the dadi forum because I really have no idea what to make of them. https://groups.google.com/forum/#!topic/dadi-user/7DHNai6wDb4

# 18 July 2019

I received a response back from Gutenkunst:

```
Hello Sarah,

Sorry to hear you’re running into this. The one standout from your script is that your passing in a list of 4 grid points for extrapolation, rather than the typical 3. So you’re doing a cubic rather than a quadratic extrapolation. It’s possible that that is less well-behaved. It certainly hasn’t been tested.

If you want to dig deeper, take one of the problematic parameter sets, evaluate the sfs for each individual grid point setting, and compare them with each other and with the eventual extrapolation. (For example, if you’re using pts=[20,30,40], you’d calculate with pts=[20], pts=[30], pts=[40] to get three spectra, then compare among them.) That will at least give you a more specific idea of where in the spectrum the problem is and what’s actually happening.

Best,
Ryan
```

So, now I have to figure out how to implement this. Some action items:

1. Obviously, I'll remove the extra pts setting -- though that shouldn't be an issue, since I added it hoping it would fix the warnings problem. [I did this, removed the 350]
2. Extract a problematic parameter set
3. Run dadi on each individual grid point setting for those parameters and compare the spectra. This step should probably be run in the interactive python environment?

From the 250_fwsw_dadi.Rmd: 

```{r dadiSetup}
knitr::opts_chunk$set(echo = TRUE,out.extra='',fig.pos="H")
knitr::opts_knit$set(root.dir='./fwsw_results/')
source("../../gwscaR/R/gwscaR.R")
source("../../gwscaR/R/gwscaR_plot.R")
source("../../gwscaR/R/gwscaR_utility.R")
source("../../gwscaR/R/gwscaR_fsts.R")
source("../../gwscaR/R/gwscaR_popgen.R")
source("../../gwscaR/R/vcf2dadi.R")
source("../R/250_dadi_analysis.R")
library(knitr)
pop.list<-c("ALFW","ALST","FLCC","FLLG","LAFW","TXCC","TXFW")
```
```{r getWarnings}
all_warnings<-lapply(list.files(pattern="251",path = "dadi_analysis",full.names = TRUE),dadi_warnings)
names(all_warnings)<-list.files(pattern="251",path = "dadi_analysis",full.names = FALSE)

#for now, focus on V1s
all_warnings<-all_warnings[c("251_2DTX_V1_1.log","251_2DTX_V1_2.log","251_2DTX_V1_3.log")] 
all_warnings<-lapply(all_warnings,function(dat) { 
  dat$Warning<-TRUE
  return(dat)
})
```

Now let's get the parameters
```{r getParams}
opt_files<-list.files(path = "dadi_analysis/TX2D",pattern="V.*optimized.*",full.names = TRUE)
opt_files<-opt_files[grep("V1",opt_files)] #focus on V1s
tx_opts<-do.call(rbind,lapply(opt_files,function(file){
  dat<-parse_dadi_opt(file)
  dat$file<-file
  return(dat)
}))
```

```{r MatchWarnings2params}
opt_warns<-do.call(rbind,mapply(function(warnings,name,opts){
  key<-gsub("251_2DTX_(V\\d)_(\\d).log","\\1_Number_\\2",name)
  dat<-merge(opts[grep(key,opts$file),],warnings,by=c("Model","Replicate"),all = TRUE)
  dat$Warning[is.na(dat$Warning)]<-FALSE
  return(dat)
},all_warnings,names(all_warnings),MoreArgs = list(opts=tx_opts),SIMPLIFY = FALSE))
```

Ok, so now I've got all the parameter combinations, let's choose a random set taht didn't work.

```{r chooseRandWarn}
opt_warns[sample(which(opt_warns$Warning==TRUE),size = 1),]
```

Ok, so looking at the manual, to run this chosen set of parameters:
                         Model            Replicate log.likelihood     AIC  chi.squared  theta                  optimized_params             params
251_2DTX_V1_2.log.243 asym_mig Round_3_Replicate_20       -1180.06 2370.12 -15550366.57 113.78 1.01,15.4299,0.9824,1.1719,3.6611 nu1,nu2,m12,m21,T.
                                                                       file Warning
251_2DTX_V1_2.log.243 dadi_analysis/TX2D/V1_Number_2.asym_mig.optimized.txt    TRUE

I need to specify the model like this:

```{python}
asym_mig([1.01,15.4299,0.9824,1.1719,3.6611], ns=[46,60] , pts=[200]) #ns are sample sizes
```


# 15 July 2019

I decided to post on the dadi forum regarding my 2D TX populations. Here is what I wrote:

```
Hi dadi community,

I'm trying to compare the fit of several models for an analysis between two populations and I'm consistently getting warnings that say
WARNING:Numerics:Extrapolation may have failed. Check resulting frequency spectrum for unexpected results.
WARNING:Inference:Model is masked in some entries where data is not.
These warnings do not go away as the model runs; they are just as likely in early runs as in later runs. Following answers to similar questions on this user group, I've tried increasing the number of points, narrowing the parameter bounds so that the model doesn't wander into difficult-to-escape parameter spaces, and using the make_extrap_func() instead of make_extrap_log_func(). However, I'm still getting these warnings pretty consistently, and not for any particular region of parameter space. I've attached a violin plot of the parameters for runs that produce warnings (left, red) and ones that don't produce warnings (right, grey) -- apologies for not making it a super pretty graph, but it conveys the idea that the parameter distributions are generally similar for the ones with warnings and the ones without. I'm getting these errors for both simple models (no migration) and complex models (ancient asymmetrical migration with population size changes). I chose the models after running 1D models for both populations, with the best fit for one population (TXCC) being growth or bottlegrowth and the best fit for the other population (TXFW) being either two_epoch or growth. The 2D frequency spectrum is attached as well.

I'm using a modified version of Daniel Portik's dadi_pipeline (https://github.com/dportik/dadi_pipeline) using the attached script. The in_params settings are ones taken from previous runs that did not produce warnings, but I get warnings whether I include those or not.

Can anyone help me understand why I'm getting these warnings and how to run the models successfully?

Thank you in advance!
```

I attached this script:
```{python, eval=FALSE}
'''
Running 2D model for TX pops
Run this from outside the dadi directory
'''

#start with ipython -pylab from ~/Research/popgen/fwsw_results/dadi_analysis/TX2D

# Numpy is the numerical library dadi is built upon
import sys
import os
import numpy
import dadi
import pylab
from datetime import datetime

#use dportik's functions
#get the optimize functions
execfile("../../programs/dadi_pipeline-master/Two_Population_Pipeline/Optimize_Functions.py")
execfile( "../../programs/dadi_pipeline-master/Two_Population_Pipeline/Models_2D.py")
execfile("../../scripts/250_custom_dadi_models.py")


# Load the data
dd = dadi.Misc.make_data_dict ( "fwsw75.dadi.snps" )
#projections is sample size of alleles
#need to use MINIMUM projections

#pops = ['FLLG', 'FLCC', 'ALFW','ALST','LAFW','TXFW','TXCC']
#projs = [70,      61,     72,     70,    72,    46,     61]

tx = dadi.Spectrum.from_data_dict(dd , pop_ids =[ 'TXFW','TXCC' ],projections =[46,60] ,polarized = False )  #polarized = False creates folded spectrum

os.chdir("TX2D")
#=================================================================================================#
#										PLOT SPECTRA	 										  #
#=================================================================================================#
dadi.Plotting.plot_single_2d_sfs(tx,vmin=0.01)


#=================================================================================================#
#										LOOP TO OPTIMIZE 										  #
#=================================================================================================#
pts = [ 200,250,300,350 ]
rounds=4
#define the lists for optional arguments
#you can change these to alter the settings of the optimization routine
reps = [10,20,30,40]
maxiters = [3,5,10,20]
folds = [3,2,2,1]
fs_folded = True
prefix = "tx"

for i in range(3,4):
	prefix = "V1_Number_{}".format(i)
	# Split into two populations, no migration.
	Optimize_Routine_Extrap(tx, pts, prefix, "no_mig", no_mig, rounds, 3, fs_folded=fs_folded, reps=reps, maxiters=maxiters, folds=folds, param_labels = "nu1, nu2, T",in_params=[1.01,9.73,0.25],in_upper=[20,20,10],in_lower=[1,1,0.01])

	# Split into two populations, with continuous symmetric migration.
	Optimize_Routine_Extrap(tx, pts, prefix, "sym_mig", sym_mig, rounds, 4, fs_folded=fs_folded, reps=reps, maxiters=maxiters, folds=folds, param_labels = "nu1, nu2, m, T",in_params=[1.01,9.77,0.60,1.09],in_upper=[20,20,10,10],in_lower=[1,1,0.01,0.01])

	# Split into two populations, with continuous asymmetric migration.
	Optimize_Routine_Extrap(tx, pts, prefix, "asym_mig", asym_mig, rounds, 5, fs_folded=fs_folded, reps=reps, maxiters=maxiters, folds=folds, param_labels = "nu1, nu2, m12, m21, T",in_params=[1.01,10.98,0.68,0.48,1.32],in_upper=[20,20,10,10,10],in_lower=[1,1,0.01,0.01,0.01])

	# Split with no migration, then instantaneous size change with no migration.
	Optimize_Routine_Extrap(tx, pts, prefix, "no_mig_size", no_mig_size, rounds, 6, fs_folded=fs_folded, reps=reps, maxiters=maxiters, folds=folds, param_labels = "nu1a, nu2a, nu1b, nu2b, T1, T2",in_params=[6.88,14.20,1.01,9.69,0.02,0.24],in_upper=[20,20,20,20,10,10],in_lower=[1,1,1,1,0.01,0.01])

	# Split with symmetric migration, then instantaneous size change with continuous symmetric migration.
	Optimize_Routine_Extrap(tx, pts, prefix, "sym_mig_size", sym_mig_size, rounds, 7, fs_folded=fs_folded, reps=reps, maxiters=maxiters, folds=folds, param_labels = "nu1a, nu2a, nu1b, nu2b, m, T1, T2",in_params=[14.75,2.53,1.01,14.93,0.50,0.98,0.60],in_upper=[20,20,20,20,10,10,10],in_lower=[1,1,1,1,0.01,0.01,0.01])

	# Split with different migration rates, then instantaneous size change with continuous asymmetric migration.
	Optimize_Routine_Extrap(tx, pts, prefix, "asym_mig_size", asym_mig_size, rounds, 8, fs_folded=fs_folded, reps=reps, maxiters=maxiters, folds=folds, param_labels = "nu1a, nu2a, nu1b, nu2b, m12, m21, T1, T2",in_params=[1.01,9,1.01,9.92,0.63,0.51,0.16,1.15],in_upper=[20,20,20,20,10,10,10,10],in_lower=[1,1,1,1,0.01,0.01,0.01,0.01])

	# Split with continuous symmetrical gene flow, followed by instantaneous size change with no migration.
	Optimize_Routine_Extrap(tx, pts, prefix, "anc_sym_mig_size", anc_sym_mig_size, rounds, 7, fs_folded=fs_folded, reps=reps, maxiters=maxiters, folds=folds, param_labels = "nu1a, nu2a, nu1b, nu2b, m, T1, T2",in_params=[1.88,10.50,1.01,15,6.95,1.77,0.24],in_upper=[20,20,20,20,10,10,10],in_lower=[1,1,1,1,0.01,0.01,0.01])

	# Split with continuous asymmetrical gene flow, followed by instantaneous size change with no migration.
	Optimize_Routine_Extrap(tx, pts, prefix, "anc_asym_mig_size", anc_asym_mig_size, rounds, 8, fs_folded=fs_folded, reps=reps, maxiters=maxiters, folds=folds, param_labels = "nu1a, nu2a, nu1b, nu2b, m12, m21, T1, T2",in_params=[1.34,7.49,1.01,15,9.76,7.66,1.34,0.25],in_upper=[20,20,20,20,10,10,10,10],in_lower=[1,1,1,1,0.01,0.01,0.01,0.01])


```

# 12 July 2019

So yesterday I did not get super far with the simulations, but today I'm going to focus first on the Texas population and dive into what parameter combinations are causing warnings. Meanwhile the 3D runs are going VERY slowly in the background.

I used existing functions (form previous analyses of FL2D runs) to visualize the parameters for each model in runs with warnings and ones without warnings. However, there are no obvious patterns arising. A bunch of them are different according to a wilcoxon rank test:
[1] "m21 is different in anc_asym_mig_size"
[1] "nu1b is different in anc_asym_mig_size"
[1] "nu2b is different in anc_asym_mig_size"
[1] "T1 is different in anc_asym_mig_size"
[1] "T2 is different in anc_asym_mig_size"
[1] "m is different in anc_sym_mig_size"
[1] "nu1b is different in anc_sym_mig_size"
[1] "nu2a is different in anc_sym_mig_size"
[1] "T2 is different in anc_sym_mig_size"
[1] "m12 is different in asym_mig"
[1] "m21 is different in asym_mig"
[1] "nu1 is different in asym_mig"
[1] "m21 is different in asym_mig_size"
[1] "nu1b is different in asym_mig_size"
[1] "nu2a is different in asym_mig_size"
[1] "T1 is different in asym_mig_size"
[1] "nu2a is different in no_mig_size"
[1] "T1 is different in no_mig_size"
[1] "m is different in sym_mig"
[1] "nu2 is different in sym_mig"
[1] "m is different in sym_mig_size"
[1] "nu1a is different in sym_mig_size"
[1] "nu2a is different in sym_mig_size"
[1] "nu2b is different in sym_mig_size"

but it's unclear to me how these issues can be fixed. All of these parameters seem to be ones that tend to be near the minimum or maximum values in parameter space.

I could try using other models -- that's what Gutenkunst suggests in one post: https://groups.google.com/forum/#!searchin/dadi-user/warning$20numerics$20extrapolation%7Csort:date/dadi-user/esRqfOQ7Amc/LfxsS0eog0IJ

# 11 July 2019

I'm not sure if this is the best way to go about doing this lab notebook thing, but I'm gonna give it a go. Today I'm trying to figure out how to analyze the $\delta_A\delta_I$ simulation results for the Florida populations. 

Portik used the simulated data to simply evaluate whether the empirical log-likelihoods and $\chi^2$ values are in the simulated distributions -- which I've done using the dadi-pipeline GOF R functions.

**How to implement heterogeneous migration, like in Tine et al. 2014 and Rougeux et al. 2017? $\delta_A\delta_I$??**
* The $\delta_A\delta_I$ user group has this response from Gutenkunst: "In your function, run through two sfs calculations, to create sfs1 and sfs2, then return `p*sfs1 + (1-p)*sfs2`."

**How to use simulations to analyze outlier data?**

**Updates on other $\delta_A\delta_I$ analyses:**
* Alabama & Louisiana analysis stopped after ~2.4 rounds of the first model results (possibly it's just slow, otherwise it could be an issue with the computer stalling)
* TX has lots of warnings - I want to delve into what's causing those by looking at the model output for the warning rounds.